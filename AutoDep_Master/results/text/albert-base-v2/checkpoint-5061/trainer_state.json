{
  "best_metric": 0.3948293626308441,
  "best_model_checkpoint": "c:\\Users\\chuvi\\Code\\SUCode\\ANLP\\anlp-project\\anlp-project\\AutoDep_Master\\results\\text\\albert-base-v2\\checkpoint-5061",
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 5061,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.005927682276229994,
      "grad_norm": 10.161124229431152,
      "learning_rate": 4.990120529539617e-05,
      "loss": 0.6908,
      "step": 10
    },
    {
      "epoch": 0.011855364552459988,
      "grad_norm": 13.26170825958252,
      "learning_rate": 4.980241059079234e-05,
      "loss": 0.6408,
      "step": 20
    },
    {
      "epoch": 0.01778304682868998,
      "grad_norm": 15.047383308410645,
      "learning_rate": 4.9703615886188506e-05,
      "loss": 0.6749,
      "step": 30
    },
    {
      "epoch": 0.023710729104919975,
      "grad_norm": 11.168242454528809,
      "learning_rate": 4.9604821181584674e-05,
      "loss": 0.6578,
      "step": 40
    },
    {
      "epoch": 0.02963841138114997,
      "grad_norm": 16.361614227294922,
      "learning_rate": 4.9506026476980835e-05,
      "loss": 0.6153,
      "step": 50
    },
    {
      "epoch": 0.03556609365737996,
      "grad_norm": 10.466825485229492,
      "learning_rate": 4.9407231772377e-05,
      "loss": 0.6618,
      "step": 60
    },
    {
      "epoch": 0.04149377593360996,
      "grad_norm": 20.217893600463867,
      "learning_rate": 4.930843706777317e-05,
      "loss": 0.6304,
      "step": 70
    },
    {
      "epoch": 0.04742145820983995,
      "grad_norm": 13.51468563079834,
      "learning_rate": 4.920964236316933e-05,
      "loss": 0.5592,
      "step": 80
    },
    {
      "epoch": 0.053349140486069944,
      "grad_norm": 9.34771728515625,
      "learning_rate": 4.91108476585655e-05,
      "loss": 0.5899,
      "step": 90
    },
    {
      "epoch": 0.05927682276229994,
      "grad_norm": 42.417335510253906,
      "learning_rate": 4.901205295396167e-05,
      "loss": 0.6186,
      "step": 100
    },
    {
      "epoch": 0.06520450503852994,
      "grad_norm": 15.10250473022461,
      "learning_rate": 4.8913258249357836e-05,
      "loss": 0.6338,
      "step": 110
    },
    {
      "epoch": 0.07113218731475993,
      "grad_norm": 250.51513671875,
      "learning_rate": 4.8814463544754004e-05,
      "loss": 0.6145,
      "step": 120
    },
    {
      "epoch": 0.07705986959098993,
      "grad_norm": 42.902496337890625,
      "learning_rate": 4.871566884015017e-05,
      "loss": 0.6367,
      "step": 130
    },
    {
      "epoch": 0.08298755186721991,
      "grad_norm": 10.545756340026855,
      "learning_rate": 4.861687413554633e-05,
      "loss": 0.6419,
      "step": 140
    },
    {
      "epoch": 0.08891523414344991,
      "grad_norm": 26.169902801513672,
      "learning_rate": 4.85180794309425e-05,
      "loss": 0.5338,
      "step": 150
    },
    {
      "epoch": 0.0948429164196799,
      "grad_norm": 19.757549285888672,
      "learning_rate": 4.841928472633867e-05,
      "loss": 0.6635,
      "step": 160
    },
    {
      "epoch": 0.1007705986959099,
      "grad_norm": 15.494487762451172,
      "learning_rate": 4.832049002173484e-05,
      "loss": 0.5874,
      "step": 170
    },
    {
      "epoch": 0.10669828097213989,
      "grad_norm": 16.69301986694336,
      "learning_rate": 4.8221695317131005e-05,
      "loss": 0.5775,
      "step": 180
    },
    {
      "epoch": 0.11262596324836989,
      "grad_norm": 7.297093391418457,
      "learning_rate": 4.812290061252717e-05,
      "loss": 0.5198,
      "step": 190
    },
    {
      "epoch": 0.11855364552459988,
      "grad_norm": 33.785987854003906,
      "learning_rate": 4.802410590792334e-05,
      "loss": 0.6286,
      "step": 200
    },
    {
      "epoch": 0.12448132780082988,
      "grad_norm": 9.313821792602539,
      "learning_rate": 4.792531120331951e-05,
      "loss": 0.6568,
      "step": 210
    },
    {
      "epoch": 0.13040901007705988,
      "grad_norm": 11.622394561767578,
      "learning_rate": 4.7826516498715677e-05,
      "loss": 0.6086,
      "step": 220
    },
    {
      "epoch": 0.13633669235328985,
      "grad_norm": 15.57612419128418,
      "learning_rate": 4.772772179411184e-05,
      "loss": 0.576,
      "step": 230
    },
    {
      "epoch": 0.14226437462951985,
      "grad_norm": 6.794140815734863,
      "learning_rate": 4.7628927089508e-05,
      "loss": 0.5607,
      "step": 240
    },
    {
      "epoch": 0.14819205690574985,
      "grad_norm": 14.009489059448242,
      "learning_rate": 4.753013238490417e-05,
      "loss": 0.5411,
      "step": 250
    },
    {
      "epoch": 0.15411973918197985,
      "grad_norm": 11.607579231262207,
      "learning_rate": 4.7431337680300335e-05,
      "loss": 0.5558,
      "step": 260
    },
    {
      "epoch": 0.16004742145820983,
      "grad_norm": 10.619332313537598,
      "learning_rate": 4.73325429756965e-05,
      "loss": 0.6315,
      "step": 270
    },
    {
      "epoch": 0.16597510373443983,
      "grad_norm": 5.460982322692871,
      "learning_rate": 4.723374827109267e-05,
      "loss": 0.6012,
      "step": 280
    },
    {
      "epoch": 0.17190278601066983,
      "grad_norm": 8.422821998596191,
      "learning_rate": 4.713495356648884e-05,
      "loss": 0.5598,
      "step": 290
    },
    {
      "epoch": 0.17783046828689983,
      "grad_norm": 6.931854248046875,
      "learning_rate": 4.7036158861885006e-05,
      "loss": 0.5793,
      "step": 300
    },
    {
      "epoch": 0.18375815056312983,
      "grad_norm": 20.177288055419922,
      "learning_rate": 4.6937364157281174e-05,
      "loss": 0.4974,
      "step": 310
    },
    {
      "epoch": 0.1896858328393598,
      "grad_norm": 9.56718635559082,
      "learning_rate": 4.6838569452677335e-05,
      "loss": 0.5469,
      "step": 320
    },
    {
      "epoch": 0.1956135151155898,
      "grad_norm": 68.67320251464844,
      "learning_rate": 4.6739774748073503e-05,
      "loss": 0.5815,
      "step": 330
    },
    {
      "epoch": 0.2015411973918198,
      "grad_norm": 122.79833221435547,
      "learning_rate": 4.664098004346967e-05,
      "loss": 0.5245,
      "step": 340
    },
    {
      "epoch": 0.2074688796680498,
      "grad_norm": 78.1559829711914,
      "learning_rate": 4.654218533886584e-05,
      "loss": 0.6321,
      "step": 350
    },
    {
      "epoch": 0.21339656194427978,
      "grad_norm": 24.242849349975586,
      "learning_rate": 4.644339063426201e-05,
      "loss": 0.5814,
      "step": 360
    },
    {
      "epoch": 0.21932424422050978,
      "grad_norm": 36.89963912963867,
      "learning_rate": 4.6344595929658175e-05,
      "loss": 0.651,
      "step": 370
    },
    {
      "epoch": 0.22525192649673978,
      "grad_norm": 20.859704971313477,
      "learning_rate": 4.624580122505434e-05,
      "loss": 0.6346,
      "step": 380
    },
    {
      "epoch": 0.23117960877296978,
      "grad_norm": 24.917713165283203,
      "learning_rate": 4.614700652045051e-05,
      "loss": 0.6302,
      "step": 390
    },
    {
      "epoch": 0.23710729104919975,
      "grad_norm": 29.016895294189453,
      "learning_rate": 4.604821181584667e-05,
      "loss": 0.5671,
      "step": 400
    },
    {
      "epoch": 0.24303497332542975,
      "grad_norm": 12.832783699035645,
      "learning_rate": 4.594941711124284e-05,
      "loss": 0.5664,
      "step": 410
    },
    {
      "epoch": 0.24896265560165975,
      "grad_norm": 11.57109546661377,
      "learning_rate": 4.5850622406639e-05,
      "loss": 0.5259,
      "step": 420
    },
    {
      "epoch": 0.2548903378778897,
      "grad_norm": 14.475279808044434,
      "learning_rate": 4.575182770203517e-05,
      "loss": 0.5819,
      "step": 430
    },
    {
      "epoch": 0.26081802015411976,
      "grad_norm": 63.90249252319336,
      "learning_rate": 4.565303299743134e-05,
      "loss": 0.6211,
      "step": 440
    },
    {
      "epoch": 0.26674570243034973,
      "grad_norm": 73.16571044921875,
      "learning_rate": 4.5554238292827505e-05,
      "loss": 0.5551,
      "step": 450
    },
    {
      "epoch": 0.2726733847065797,
      "grad_norm": 36.59919357299805,
      "learning_rate": 4.545544358822367e-05,
      "loss": 0.5478,
      "step": 460
    },
    {
      "epoch": 0.27860106698280973,
      "grad_norm": 27.131576538085938,
      "learning_rate": 4.535664888361984e-05,
      "loss": 0.6151,
      "step": 470
    },
    {
      "epoch": 0.2845287492590397,
      "grad_norm": 14.471365928649902,
      "learning_rate": 4.525785417901601e-05,
      "loss": 0.5712,
      "step": 480
    },
    {
      "epoch": 0.29045643153526973,
      "grad_norm": 13.836601257324219,
      "learning_rate": 4.515905947441218e-05,
      "loss": 0.5986,
      "step": 490
    },
    {
      "epoch": 0.2963841138114997,
      "grad_norm": 9.851883888244629,
      "learning_rate": 4.5060264769808345e-05,
      "loss": 0.6126,
      "step": 500
    },
    {
      "epoch": 0.3023117960877297,
      "grad_norm": 8.354625701904297,
      "learning_rate": 4.4961470065204506e-05,
      "loss": 0.521,
      "step": 510
    },
    {
      "epoch": 0.3082394783639597,
      "grad_norm": 19.55716323852539,
      "learning_rate": 4.4862675360600674e-05,
      "loss": 0.5017,
      "step": 520
    },
    {
      "epoch": 0.3141671606401897,
      "grad_norm": 17.373065948486328,
      "learning_rate": 4.476388065599684e-05,
      "loss": 0.4918,
      "step": 530
    },
    {
      "epoch": 0.32009484291641965,
      "grad_norm": 12.192534446716309,
      "learning_rate": 4.466508595139301e-05,
      "loss": 0.5234,
      "step": 540
    },
    {
      "epoch": 0.3260225251926497,
      "grad_norm": 21.954313278198242,
      "learning_rate": 4.456629124678918e-05,
      "loss": 0.5151,
      "step": 550
    },
    {
      "epoch": 0.33195020746887965,
      "grad_norm": 16.208221435546875,
      "learning_rate": 4.446749654218534e-05,
      "loss": 0.5937,
      "step": 560
    },
    {
      "epoch": 0.3378778897451097,
      "grad_norm": 55.068504333496094,
      "learning_rate": 4.436870183758151e-05,
      "loss": 0.5697,
      "step": 570
    },
    {
      "epoch": 0.34380557202133966,
      "grad_norm": 45.51179122924805,
      "learning_rate": 4.4269907132977675e-05,
      "loss": 0.5129,
      "step": 580
    },
    {
      "epoch": 0.34973325429756963,
      "grad_norm": 12.437984466552734,
      "learning_rate": 4.417111242837384e-05,
      "loss": 0.6185,
      "step": 590
    },
    {
      "epoch": 0.35566093657379966,
      "grad_norm": 18.581085205078125,
      "learning_rate": 4.4072317723770004e-05,
      "loss": 0.5165,
      "step": 600
    },
    {
      "epoch": 0.36158861885002963,
      "grad_norm": 23.36187744140625,
      "learning_rate": 4.397352301916617e-05,
      "loss": 0.5115,
      "step": 610
    },
    {
      "epoch": 0.36751630112625966,
      "grad_norm": 13.504555702209473,
      "learning_rate": 4.387472831456234e-05,
      "loss": 0.5639,
      "step": 620
    },
    {
      "epoch": 0.37344398340248963,
      "grad_norm": 31.79749870300293,
      "learning_rate": 4.377593360995851e-05,
      "loss": 0.5043,
      "step": 630
    },
    {
      "epoch": 0.3793716656787196,
      "grad_norm": 38.95268249511719,
      "learning_rate": 4.3677138905354675e-05,
      "loss": 0.5202,
      "step": 640
    },
    {
      "epoch": 0.38529934795494963,
      "grad_norm": 24.532787322998047,
      "learning_rate": 4.357834420075084e-05,
      "loss": 0.4589,
      "step": 650
    },
    {
      "epoch": 0.3912270302311796,
      "grad_norm": 12.012605667114258,
      "learning_rate": 4.347954949614701e-05,
      "loss": 0.5539,
      "step": 660
    },
    {
      "epoch": 0.3971547125074096,
      "grad_norm": 11.958657264709473,
      "learning_rate": 4.338075479154318e-05,
      "loss": 0.4616,
      "step": 670
    },
    {
      "epoch": 0.4030823947836396,
      "grad_norm": 40.62808609008789,
      "learning_rate": 4.328196008693935e-05,
      "loss": 0.5088,
      "step": 680
    },
    {
      "epoch": 0.4090100770598696,
      "grad_norm": 47.91569900512695,
      "learning_rate": 4.318316538233551e-05,
      "loss": 0.5183,
      "step": 690
    },
    {
      "epoch": 0.4149377593360996,
      "grad_norm": 16.3569278717041,
      "learning_rate": 4.3084370677731676e-05,
      "loss": 0.5828,
      "step": 700
    },
    {
      "epoch": 0.4208654416123296,
      "grad_norm": 47.268375396728516,
      "learning_rate": 4.2985575973127844e-05,
      "loss": 0.4905,
      "step": 710
    },
    {
      "epoch": 0.42679312388855956,
      "grad_norm": 71.96983337402344,
      "learning_rate": 4.2886781268524005e-05,
      "loss": 0.5331,
      "step": 720
    },
    {
      "epoch": 0.4327208061647896,
      "grad_norm": 5.641787528991699,
      "learning_rate": 4.278798656392017e-05,
      "loss": 0.6428,
      "step": 730
    },
    {
      "epoch": 0.43864848844101956,
      "grad_norm": 51.863365173339844,
      "learning_rate": 4.268919185931634e-05,
      "loss": 0.4849,
      "step": 740
    },
    {
      "epoch": 0.44457617071724953,
      "grad_norm": 37.18937301635742,
      "learning_rate": 4.259039715471251e-05,
      "loss": 0.5447,
      "step": 750
    },
    {
      "epoch": 0.45050385299347956,
      "grad_norm": 11.722724914550781,
      "learning_rate": 4.249160245010868e-05,
      "loss": 0.5476,
      "step": 760
    },
    {
      "epoch": 0.45643153526970953,
      "grad_norm": 43.98824691772461,
      "learning_rate": 4.2392807745504845e-05,
      "loss": 0.6262,
      "step": 770
    },
    {
      "epoch": 0.46235921754593956,
      "grad_norm": 8.402789115905762,
      "learning_rate": 4.2294013040901006e-05,
      "loss": 0.5162,
      "step": 780
    },
    {
      "epoch": 0.46828689982216953,
      "grad_norm": 33.1689567565918,
      "learning_rate": 4.2195218336297174e-05,
      "loss": 0.5358,
      "step": 790
    },
    {
      "epoch": 0.4742145820983995,
      "grad_norm": 15.662578582763672,
      "learning_rate": 4.209642363169334e-05,
      "loss": 0.4971,
      "step": 800
    },
    {
      "epoch": 0.48014226437462953,
      "grad_norm": 32.71614074707031,
      "learning_rate": 4.199762892708951e-05,
      "loss": 0.4791,
      "step": 810
    },
    {
      "epoch": 0.4860699466508595,
      "grad_norm": 13.527876853942871,
      "learning_rate": 4.189883422248568e-05,
      "loss": 0.4655,
      "step": 820
    },
    {
      "epoch": 0.4919976289270895,
      "grad_norm": 8.002348899841309,
      "learning_rate": 4.1800039517881846e-05,
      "loss": 0.5425,
      "step": 830
    },
    {
      "epoch": 0.4979253112033195,
      "grad_norm": 42.055450439453125,
      "learning_rate": 4.1701244813278014e-05,
      "loss": 0.5944,
      "step": 840
    },
    {
      "epoch": 0.5038529934795495,
      "grad_norm": 6.069934368133545,
      "learning_rate": 4.160245010867418e-05,
      "loss": 0.6239,
      "step": 850
    },
    {
      "epoch": 0.5097806757557795,
      "grad_norm": 6.783214569091797,
      "learning_rate": 4.150365540407035e-05,
      "loss": 0.65,
      "step": 860
    },
    {
      "epoch": 0.5157083580320095,
      "grad_norm": 75.57958221435547,
      "learning_rate": 4.140486069946651e-05,
      "loss": 0.4999,
      "step": 870
    },
    {
      "epoch": 0.5216360403082395,
      "grad_norm": 14.70267105102539,
      "learning_rate": 4.130606599486267e-05,
      "loss": 0.5237,
      "step": 880
    },
    {
      "epoch": 0.5275637225844695,
      "grad_norm": 49.0606803894043,
      "learning_rate": 4.120727129025884e-05,
      "loss": 0.5604,
      "step": 890
    },
    {
      "epoch": 0.5334914048606995,
      "grad_norm": 5.73143196105957,
      "learning_rate": 4.110847658565501e-05,
      "loss": 0.5571,
      "step": 900
    },
    {
      "epoch": 0.5394190871369294,
      "grad_norm": 8.64951229095459,
      "learning_rate": 4.1009681881051176e-05,
      "loss": 0.5124,
      "step": 910
    },
    {
      "epoch": 0.5453467694131594,
      "grad_norm": 25.14156150817871,
      "learning_rate": 4.0910887176447343e-05,
      "loss": 0.6028,
      "step": 920
    },
    {
      "epoch": 0.5512744516893895,
      "grad_norm": 6.401345729827881,
      "learning_rate": 4.081209247184351e-05,
      "loss": 0.5774,
      "step": 930
    },
    {
      "epoch": 0.5572021339656195,
      "grad_norm": 8.712220191955566,
      "learning_rate": 4.071329776723968e-05,
      "loss": 0.5753,
      "step": 940
    },
    {
      "epoch": 0.5631298162418494,
      "grad_norm": 11.452255249023438,
      "learning_rate": 4.061450306263585e-05,
      "loss": 0.5115,
      "step": 950
    },
    {
      "epoch": 0.5690574985180794,
      "grad_norm": 10.3247652053833,
      "learning_rate": 4.051570835803201e-05,
      "loss": 0.457,
      "step": 960
    },
    {
      "epoch": 0.5749851807943094,
      "grad_norm": 8.051189422607422,
      "learning_rate": 4.0416913653428176e-05,
      "loss": 0.6535,
      "step": 970
    },
    {
      "epoch": 0.5809128630705395,
      "grad_norm": 7.823803901672363,
      "learning_rate": 4.0318118948824344e-05,
      "loss": 0.5654,
      "step": 980
    },
    {
      "epoch": 0.5868405453467694,
      "grad_norm": 2.754333019256592,
      "learning_rate": 4.021932424422051e-05,
      "loss": 0.561,
      "step": 990
    },
    {
      "epoch": 0.5927682276229994,
      "grad_norm": 3.2010560035705566,
      "learning_rate": 4.012052953961668e-05,
      "loss": 0.6191,
      "step": 1000
    },
    {
      "epoch": 0.5986959098992294,
      "grad_norm": 20.963420867919922,
      "learning_rate": 4.002173483501285e-05,
      "loss": 0.5632,
      "step": 1010
    },
    {
      "epoch": 0.6046235921754594,
      "grad_norm": 7.515842437744141,
      "learning_rate": 3.9922940130409016e-05,
      "loss": 0.561,
      "step": 1020
    },
    {
      "epoch": 0.6105512744516894,
      "grad_norm": 58.44517517089844,
      "learning_rate": 3.982414542580518e-05,
      "loss": 0.5497,
      "step": 1030
    },
    {
      "epoch": 0.6164789567279194,
      "grad_norm": 18.94573211669922,
      "learning_rate": 3.9725350721201345e-05,
      "loss": 0.4113,
      "step": 1040
    },
    {
      "epoch": 0.6224066390041494,
      "grad_norm": 238.8856964111328,
      "learning_rate": 3.962655601659751e-05,
      "loss": 0.518,
      "step": 1050
    },
    {
      "epoch": 0.6283343212803794,
      "grad_norm": 101.71446990966797,
      "learning_rate": 3.9527761311993674e-05,
      "loss": 0.5327,
      "step": 1060
    },
    {
      "epoch": 0.6342620035566093,
      "grad_norm": 6.219387054443359,
      "learning_rate": 3.942896660738984e-05,
      "loss": 0.5326,
      "step": 1070
    },
    {
      "epoch": 0.6401896858328393,
      "grad_norm": 5.771879196166992,
      "learning_rate": 3.933017190278601e-05,
      "loss": 0.4716,
      "step": 1080
    },
    {
      "epoch": 0.6461173681090694,
      "grad_norm": 32.685272216796875,
      "learning_rate": 3.923137719818218e-05,
      "loss": 0.5534,
      "step": 1090
    },
    {
      "epoch": 0.6520450503852994,
      "grad_norm": 19.75934410095215,
      "learning_rate": 3.9132582493578346e-05,
      "loss": 0.4991,
      "step": 1100
    },
    {
      "epoch": 0.6579727326615293,
      "grad_norm": 17.99289894104004,
      "learning_rate": 3.9033787788974514e-05,
      "loss": 0.576,
      "step": 1110
    },
    {
      "epoch": 0.6639004149377593,
      "grad_norm": 38.65797805786133,
      "learning_rate": 3.893499308437068e-05,
      "loss": 0.4983,
      "step": 1120
    },
    {
      "epoch": 0.6698280972139893,
      "grad_norm": 6.34208345413208,
      "learning_rate": 3.883619837976685e-05,
      "loss": 0.4788,
      "step": 1130
    },
    {
      "epoch": 0.6757557794902194,
      "grad_norm": 16.977649688720703,
      "learning_rate": 3.873740367516302e-05,
      "loss": 0.4308,
      "step": 1140
    },
    {
      "epoch": 0.6816834617664493,
      "grad_norm": 10.427937507629395,
      "learning_rate": 3.863860897055918e-05,
      "loss": 0.5163,
      "step": 1150
    },
    {
      "epoch": 0.6876111440426793,
      "grad_norm": 228.77536010742188,
      "learning_rate": 3.853981426595535e-05,
      "loss": 0.5197,
      "step": 1160
    },
    {
      "epoch": 0.6935388263189093,
      "grad_norm": 12.161484718322754,
      "learning_rate": 3.8441019561351515e-05,
      "loss": 0.5464,
      "step": 1170
    },
    {
      "epoch": 0.6994665085951393,
      "grad_norm": 6.431323528289795,
      "learning_rate": 3.834222485674768e-05,
      "loss": 0.457,
      "step": 1180
    },
    {
      "epoch": 0.7053941908713693,
      "grad_norm": 4.493706226348877,
      "learning_rate": 3.8243430152143844e-05,
      "loss": 0.6127,
      "step": 1190
    },
    {
      "epoch": 0.7113218731475993,
      "grad_norm": 104.3836441040039,
      "learning_rate": 3.814463544754001e-05,
      "loss": 0.5101,
      "step": 1200
    },
    {
      "epoch": 0.7172495554238293,
      "grad_norm": 5.14698600769043,
      "learning_rate": 3.804584074293618e-05,
      "loss": 0.4577,
      "step": 1210
    },
    {
      "epoch": 0.7231772377000593,
      "grad_norm": 23.63407325744629,
      "learning_rate": 3.794704603833235e-05,
      "loss": 0.4613,
      "step": 1220
    },
    {
      "epoch": 0.7291049199762892,
      "grad_norm": 11.728093147277832,
      "learning_rate": 3.7848251333728515e-05,
      "loss": 0.4911,
      "step": 1230
    },
    {
      "epoch": 0.7350326022525193,
      "grad_norm": 45.91849899291992,
      "learning_rate": 3.7749456629124677e-05,
      "loss": 0.485,
      "step": 1240
    },
    {
      "epoch": 0.7409602845287493,
      "grad_norm": 12.272936820983887,
      "learning_rate": 3.7650661924520845e-05,
      "loss": 0.468,
      "step": 1250
    },
    {
      "epoch": 0.7468879668049793,
      "grad_norm": 18.5985050201416,
      "learning_rate": 3.755186721991701e-05,
      "loss": 0.5322,
      "step": 1260
    },
    {
      "epoch": 0.7528156490812092,
      "grad_norm": 29.861644744873047,
      "learning_rate": 3.745307251531318e-05,
      "loss": 0.5181,
      "step": 1270
    },
    {
      "epoch": 0.7587433313574392,
      "grad_norm": 13.890777587890625,
      "learning_rate": 3.735427781070935e-05,
      "loss": 0.4041,
      "step": 1280
    },
    {
      "epoch": 0.7646710136336692,
      "grad_norm": 48.92064666748047,
      "learning_rate": 3.7255483106105516e-05,
      "loss": 0.5757,
      "step": 1290
    },
    {
      "epoch": 0.7705986959098993,
      "grad_norm": 11.369391441345215,
      "learning_rate": 3.7156688401501684e-05,
      "loss": 0.5061,
      "step": 1300
    },
    {
      "epoch": 0.7765263781861292,
      "grad_norm": 4.402633190155029,
      "learning_rate": 3.705789369689785e-05,
      "loss": 0.5023,
      "step": 1310
    },
    {
      "epoch": 0.7824540604623592,
      "grad_norm": 31.41122055053711,
      "learning_rate": 3.695909899229402e-05,
      "loss": 0.5182,
      "step": 1320
    },
    {
      "epoch": 0.7883817427385892,
      "grad_norm": 16.837535858154297,
      "learning_rate": 3.686030428769018e-05,
      "loss": 0.6012,
      "step": 1330
    },
    {
      "epoch": 0.7943094250148192,
      "grad_norm": 17.68408966064453,
      "learning_rate": 3.676150958308635e-05,
      "loss": 0.5181,
      "step": 1340
    },
    {
      "epoch": 0.8002371072910492,
      "grad_norm": 28.281538009643555,
      "learning_rate": 3.666271487848251e-05,
      "loss": 0.5188,
      "step": 1350
    },
    {
      "epoch": 0.8061647895672792,
      "grad_norm": 9.940011024475098,
      "learning_rate": 3.656392017387868e-05,
      "loss": 0.4345,
      "step": 1360
    },
    {
      "epoch": 0.8120924718435092,
      "grad_norm": 293.8431701660156,
      "learning_rate": 3.6465125469274846e-05,
      "loss": 0.4613,
      "step": 1370
    },
    {
      "epoch": 0.8180201541197392,
      "grad_norm": 20.543962478637695,
      "learning_rate": 3.6366330764671014e-05,
      "loss": 0.4853,
      "step": 1380
    },
    {
      "epoch": 0.8239478363959691,
      "grad_norm": 29.52580451965332,
      "learning_rate": 3.626753606006718e-05,
      "loss": 0.5348,
      "step": 1390
    },
    {
      "epoch": 0.8298755186721992,
      "grad_norm": 87.13895416259766,
      "learning_rate": 3.616874135546335e-05,
      "loss": 0.4216,
      "step": 1400
    },
    {
      "epoch": 0.8358032009484292,
      "grad_norm": 7.074516773223877,
      "learning_rate": 3.606994665085952e-05,
      "loss": 0.5355,
      "step": 1410
    },
    {
      "epoch": 0.8417308832246592,
      "grad_norm": 323.1661376953125,
      "learning_rate": 3.597115194625568e-05,
      "loss": 0.4674,
      "step": 1420
    },
    {
      "epoch": 0.8476585655008891,
      "grad_norm": 11.765792846679688,
      "learning_rate": 3.587235724165185e-05,
      "loss": 0.5105,
      "step": 1430
    },
    {
      "epoch": 0.8535862477771191,
      "grad_norm": 13.943761825561523,
      "learning_rate": 3.5773562537048015e-05,
      "loss": 0.506,
      "step": 1440
    },
    {
      "epoch": 0.8595139300533492,
      "grad_norm": 7.428855895996094,
      "learning_rate": 3.567476783244418e-05,
      "loss": 0.5411,
      "step": 1450
    },
    {
      "epoch": 0.8654416123295792,
      "grad_norm": 7.060905456542969,
      "learning_rate": 3.557597312784035e-05,
      "loss": 0.4916,
      "step": 1460
    },
    {
      "epoch": 0.8713692946058091,
      "grad_norm": 6.235435485839844,
      "learning_rate": 3.547717842323652e-05,
      "loss": 0.3408,
      "step": 1470
    },
    {
      "epoch": 0.8772969768820391,
      "grad_norm": 12.985630989074707,
      "learning_rate": 3.5378383718632687e-05,
      "loss": 0.5413,
      "step": 1480
    },
    {
      "epoch": 0.8832246591582691,
      "grad_norm": 16.37630844116211,
      "learning_rate": 3.5279589014028855e-05,
      "loss": 0.5489,
      "step": 1490
    },
    {
      "epoch": 0.8891523414344991,
      "grad_norm": 6.321455001831055,
      "learning_rate": 3.518079430942502e-05,
      "loss": 0.4749,
      "step": 1500
    },
    {
      "epoch": 0.8950800237107291,
      "grad_norm": 164.0301055908203,
      "learning_rate": 3.5081999604821184e-05,
      "loss": 0.4344,
      "step": 1510
    },
    {
      "epoch": 0.9010077059869591,
      "grad_norm": 5.860137939453125,
      "learning_rate": 3.4983204900217345e-05,
      "loss": 0.4825,
      "step": 1520
    },
    {
      "epoch": 0.9069353882631891,
      "grad_norm": 6.089977741241455,
      "learning_rate": 3.488441019561351e-05,
      "loss": 0.4533,
      "step": 1530
    },
    {
      "epoch": 0.9128630705394191,
      "grad_norm": 24.377971649169922,
      "learning_rate": 3.478561549100968e-05,
      "loss": 0.4752,
      "step": 1540
    },
    {
      "epoch": 0.918790752815649,
      "grad_norm": 20.785724639892578,
      "learning_rate": 3.468682078640585e-05,
      "loss": 0.5295,
      "step": 1550
    },
    {
      "epoch": 0.9247184350918791,
      "grad_norm": 57.24374771118164,
      "learning_rate": 3.4588026081802016e-05,
      "loss": 0.7653,
      "step": 1560
    },
    {
      "epoch": 0.9306461173681091,
      "grad_norm": 5.909299850463867,
      "learning_rate": 3.4489231377198184e-05,
      "loss": 0.5072,
      "step": 1570
    },
    {
      "epoch": 0.9365737996443391,
      "grad_norm": 745.7210083007812,
      "learning_rate": 3.439043667259435e-05,
      "loss": 0.4866,
      "step": 1580
    },
    {
      "epoch": 0.942501481920569,
      "grad_norm": 7.1468329429626465,
      "learning_rate": 3.429164196799052e-05,
      "loss": 0.4974,
      "step": 1590
    },
    {
      "epoch": 0.948429164196799,
      "grad_norm": 8.773387908935547,
      "learning_rate": 3.419284726338668e-05,
      "loss": 0.5335,
      "step": 1600
    },
    {
      "epoch": 0.9543568464730291,
      "grad_norm": 13.26337718963623,
      "learning_rate": 3.409405255878285e-05,
      "loss": 0.4622,
      "step": 1610
    },
    {
      "epoch": 0.9602845287492591,
      "grad_norm": 8.9894380569458,
      "learning_rate": 3.399525785417902e-05,
      "loss": 0.5418,
      "step": 1620
    },
    {
      "epoch": 0.966212211025489,
      "grad_norm": 21.272056579589844,
      "learning_rate": 3.3896463149575185e-05,
      "loss": 0.5654,
      "step": 1630
    },
    {
      "epoch": 0.972139893301719,
      "grad_norm": 6.448025226593018,
      "learning_rate": 3.379766844497135e-05,
      "loss": 0.5242,
      "step": 1640
    },
    {
      "epoch": 0.978067575577949,
      "grad_norm": 6.166467189788818,
      "learning_rate": 3.369887374036752e-05,
      "loss": 0.5178,
      "step": 1650
    },
    {
      "epoch": 0.983995257854179,
      "grad_norm": 290.25091552734375,
      "learning_rate": 3.360007903576369e-05,
      "loss": 0.4835,
      "step": 1660
    },
    {
      "epoch": 0.989922940130409,
      "grad_norm": 9.350057601928711,
      "learning_rate": 3.350128433115985e-05,
      "loss": 0.537,
      "step": 1670
    },
    {
      "epoch": 0.995850622406639,
      "grad_norm": 6.101749897003174,
      "learning_rate": 3.340248962655602e-05,
      "loss": 0.4872,
      "step": 1680
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.7745330566261488,
      "eval_f1": 0.7786992579659537,
      "eval_loss": 0.4941551387310028,
      "eval_precision": 0.6907589055240062,
      "eval_recall": 0.8922974324774925,
      "eval_runtime": 13.676,
      "eval_samples_per_second": 493.272,
      "eval_steps_per_second": 30.857,
      "step": 1687
    },
    {
      "epoch": 1.001778304682869,
      "grad_norm": 3.759908437728882,
      "learning_rate": 3.3303694921952186e-05,
      "loss": 0.5389,
      "step": 1690
    },
    {
      "epoch": 1.007705986959099,
      "grad_norm": 8.236708641052246,
      "learning_rate": 3.320490021734835e-05,
      "loss": 0.4566,
      "step": 1700
    },
    {
      "epoch": 1.013633669235329,
      "grad_norm": 69.06660461425781,
      "learning_rate": 3.3106105512744515e-05,
      "loss": 0.5285,
      "step": 1710
    },
    {
      "epoch": 1.019561351511559,
      "grad_norm": 16.272361755371094,
      "learning_rate": 3.300731080814068e-05,
      "loss": 0.4837,
      "step": 1720
    },
    {
      "epoch": 1.0254890337877889,
      "grad_norm": 14.366669654846191,
      "learning_rate": 3.290851610353685e-05,
      "loss": 0.3924,
      "step": 1730
    },
    {
      "epoch": 1.0314167160640189,
      "grad_norm": 20.169204711914062,
      "learning_rate": 3.280972139893302e-05,
      "loss": 0.4331,
      "step": 1740
    },
    {
      "epoch": 1.037344398340249,
      "grad_norm": 11.239407539367676,
      "learning_rate": 3.271092669432919e-05,
      "loss": 0.4211,
      "step": 1750
    },
    {
      "epoch": 1.043272080616479,
      "grad_norm": 36.23087692260742,
      "learning_rate": 3.2612131989725355e-05,
      "loss": 0.4254,
      "step": 1760
    },
    {
      "epoch": 1.049199762892709,
      "grad_norm": 49.828895568847656,
      "learning_rate": 3.251333728512152e-05,
      "loss": 0.3081,
      "step": 1770
    },
    {
      "epoch": 1.055127445168939,
      "grad_norm": 59.408931732177734,
      "learning_rate": 3.241454258051769e-05,
      "loss": 0.4051,
      "step": 1780
    },
    {
      "epoch": 1.061055127445169,
      "grad_norm": 30.569923400878906,
      "learning_rate": 3.231574787591385e-05,
      "loss": 0.5528,
      "step": 1790
    },
    {
      "epoch": 1.066982809721399,
      "grad_norm": 10.299112319946289,
      "learning_rate": 3.221695317131002e-05,
      "loss": 0.4943,
      "step": 1800
    },
    {
      "epoch": 1.072910491997629,
      "grad_norm": 9.296185493469238,
      "learning_rate": 3.211815846670619e-05,
      "loss": 0.4128,
      "step": 1810
    },
    {
      "epoch": 1.0788381742738589,
      "grad_norm": 14.046798706054688,
      "learning_rate": 3.201936376210235e-05,
      "loss": 0.4914,
      "step": 1820
    },
    {
      "epoch": 1.0847658565500888,
      "grad_norm": 28.382823944091797,
      "learning_rate": 3.192056905749852e-05,
      "loss": 0.5306,
      "step": 1830
    },
    {
      "epoch": 1.0906935388263188,
      "grad_norm": 9.328556060791016,
      "learning_rate": 3.1821774352894685e-05,
      "loss": 0.4574,
      "step": 1840
    },
    {
      "epoch": 1.096621221102549,
      "grad_norm": 28.29318618774414,
      "learning_rate": 3.172297964829085e-05,
      "loss": 0.4663,
      "step": 1850
    },
    {
      "epoch": 1.102548903378779,
      "grad_norm": 23.488359451293945,
      "learning_rate": 3.162418494368702e-05,
      "loss": 0.4443,
      "step": 1860
    },
    {
      "epoch": 1.108476585655009,
      "grad_norm": 8.609654426574707,
      "learning_rate": 3.152539023908319e-05,
      "loss": 0.4893,
      "step": 1870
    },
    {
      "epoch": 1.114404267931239,
      "grad_norm": 27.786441802978516,
      "learning_rate": 3.142659553447935e-05,
      "loss": 0.5285,
      "step": 1880
    },
    {
      "epoch": 1.120331950207469,
      "grad_norm": 9.73981761932373,
      "learning_rate": 3.132780082987552e-05,
      "loss": 0.5305,
      "step": 1890
    },
    {
      "epoch": 1.1262596324836989,
      "grad_norm": 8.289258003234863,
      "learning_rate": 3.1229006125271685e-05,
      "loss": 0.4891,
      "step": 1900
    },
    {
      "epoch": 1.1321873147599288,
      "grad_norm": 77.18204498291016,
      "learning_rate": 3.113021142066785e-05,
      "loss": 0.5287,
      "step": 1910
    },
    {
      "epoch": 1.1381149970361588,
      "grad_norm": 12.229536056518555,
      "learning_rate": 3.103141671606402e-05,
      "loss": 0.502,
      "step": 1920
    },
    {
      "epoch": 1.1440426793123888,
      "grad_norm": 13.459188461303711,
      "learning_rate": 3.093262201146019e-05,
      "loss": 0.5106,
      "step": 1930
    },
    {
      "epoch": 1.1499703615886188,
      "grad_norm": 16.00262451171875,
      "learning_rate": 3.083382730685636e-05,
      "loss": 0.4565,
      "step": 1940
    },
    {
      "epoch": 1.1558980438648487,
      "grad_norm": 8.601886749267578,
      "learning_rate": 3.0735032602252525e-05,
      "loss": 0.4935,
      "step": 1950
    },
    {
      "epoch": 1.161825726141079,
      "grad_norm": 17.20250701904297,
      "learning_rate": 3.063623789764869e-05,
      "loss": 0.4026,
      "step": 1960
    },
    {
      "epoch": 1.167753408417309,
      "grad_norm": 9.497243881225586,
      "learning_rate": 3.0537443193044854e-05,
      "loss": 0.4589,
      "step": 1970
    },
    {
      "epoch": 1.1736810906935389,
      "grad_norm": 6.127623558044434,
      "learning_rate": 3.043864848844102e-05,
      "loss": 0.3799,
      "step": 1980
    },
    {
      "epoch": 1.1796087729697688,
      "grad_norm": 26.308931350708008,
      "learning_rate": 3.0339853783837187e-05,
      "loss": 0.44,
      "step": 1990
    },
    {
      "epoch": 1.1855364552459988,
      "grad_norm": 29.8370418548584,
      "learning_rate": 3.024105907923335e-05,
      "loss": 0.4213,
      "step": 2000
    },
    {
      "epoch": 1.1914641375222288,
      "grad_norm": 17.87353515625,
      "learning_rate": 3.014226437462952e-05,
      "loss": 0.4491,
      "step": 2010
    },
    {
      "epoch": 1.1973918197984588,
      "grad_norm": 14.055922508239746,
      "learning_rate": 3.0043469670025687e-05,
      "loss": 0.506,
      "step": 2020
    },
    {
      "epoch": 1.2033195020746887,
      "grad_norm": 7.607759475708008,
      "learning_rate": 2.9944674965421855e-05,
      "loss": 0.3684,
      "step": 2030
    },
    {
      "epoch": 1.2092471843509187,
      "grad_norm": 26.028057098388672,
      "learning_rate": 2.984588026081802e-05,
      "loss": 0.5432,
      "step": 2040
    },
    {
      "epoch": 1.215174866627149,
      "grad_norm": 157.5063018798828,
      "learning_rate": 2.9747085556214187e-05,
      "loss": 0.5399,
      "step": 2050
    },
    {
      "epoch": 1.2211025489033789,
      "grad_norm": 17.382211685180664,
      "learning_rate": 2.9648290851610355e-05,
      "loss": 0.4722,
      "step": 2060
    },
    {
      "epoch": 1.2270302311796089,
      "grad_norm": 52.982967376708984,
      "learning_rate": 2.9549496147006523e-05,
      "loss": 0.457,
      "step": 2070
    },
    {
      "epoch": 1.2329579134558388,
      "grad_norm": 20.936004638671875,
      "learning_rate": 2.9450701442402688e-05,
      "loss": 0.5055,
      "step": 2080
    },
    {
      "epoch": 1.2388855957320688,
      "grad_norm": 13.01406478881836,
      "learning_rate": 2.9351906737798856e-05,
      "loss": 0.4871,
      "step": 2090
    },
    {
      "epoch": 1.2448132780082988,
      "grad_norm": 3.669673204421997,
      "learning_rate": 2.9253112033195024e-05,
      "loss": 0.4411,
      "step": 2100
    },
    {
      "epoch": 1.2507409602845287,
      "grad_norm": 20.690616607666016,
      "learning_rate": 2.915431732859119e-05,
      "loss": 0.3809,
      "step": 2110
    },
    {
      "epoch": 1.2566686425607587,
      "grad_norm": 6.880071640014648,
      "learning_rate": 2.905552262398736e-05,
      "loss": 0.5685,
      "step": 2120
    },
    {
      "epoch": 1.2625963248369887,
      "grad_norm": 33.478763580322266,
      "learning_rate": 2.8956727919383524e-05,
      "loss": 0.4663,
      "step": 2130
    },
    {
      "epoch": 1.2685240071132187,
      "grad_norm": 7.821342468261719,
      "learning_rate": 2.8857933214779685e-05,
      "loss": 0.4236,
      "step": 2140
    },
    {
      "epoch": 1.2744516893894486,
      "grad_norm": 11.465217590332031,
      "learning_rate": 2.8759138510175853e-05,
      "loss": 0.6586,
      "step": 2150
    },
    {
      "epoch": 1.2803793716656786,
      "grad_norm": 19.353574752807617,
      "learning_rate": 2.866034380557202e-05,
      "loss": 0.4921,
      "step": 2160
    },
    {
      "epoch": 1.2863070539419086,
      "grad_norm": 16.753293991088867,
      "learning_rate": 2.856154910096819e-05,
      "loss": 0.4853,
      "step": 2170
    },
    {
      "epoch": 1.2922347362181388,
      "grad_norm": 16.991769790649414,
      "learning_rate": 2.8462754396364354e-05,
      "loss": 0.4044,
      "step": 2180
    },
    {
      "epoch": 1.2981624184943688,
      "grad_norm": 4.9346604347229,
      "learning_rate": 2.836395969176052e-05,
      "loss": 0.4848,
      "step": 2190
    },
    {
      "epoch": 1.3040901007705987,
      "grad_norm": 14.47732925415039,
      "learning_rate": 2.826516498715669e-05,
      "loss": 0.3496,
      "step": 2200
    },
    {
      "epoch": 1.3100177830468287,
      "grad_norm": 11.453994750976562,
      "learning_rate": 2.8166370282552857e-05,
      "loss": 0.3892,
      "step": 2210
    },
    {
      "epoch": 1.3159454653230587,
      "grad_norm": 14.126304626464844,
      "learning_rate": 2.8067575577949022e-05,
      "loss": 0.5153,
      "step": 2220
    },
    {
      "epoch": 1.3218731475992886,
      "grad_norm": 10.463335037231445,
      "learning_rate": 2.796878087334519e-05,
      "loss": 0.3977,
      "step": 2230
    },
    {
      "epoch": 1.3278008298755186,
      "grad_norm": 11.16727066040039,
      "learning_rate": 2.7869986168741358e-05,
      "loss": 0.439,
      "step": 2240
    },
    {
      "epoch": 1.3337285121517486,
      "grad_norm": 25.198686599731445,
      "learning_rate": 2.7771191464137526e-05,
      "loss": 0.384,
      "step": 2250
    },
    {
      "epoch": 1.3396561944279788,
      "grad_norm": 10.64380168914795,
      "learning_rate": 2.7672396759533694e-05,
      "loss": 0.5976,
      "step": 2260
    },
    {
      "epoch": 1.3455838767042088,
      "grad_norm": 7.170768737792969,
      "learning_rate": 2.7573602054929858e-05,
      "loss": 0.3326,
      "step": 2270
    },
    {
      "epoch": 1.3515115589804387,
      "grad_norm": 9.482901573181152,
      "learning_rate": 2.7474807350326026e-05,
      "loss": 0.4435,
      "step": 2280
    },
    {
      "epoch": 1.3574392412566687,
      "grad_norm": 20.649642944335938,
      "learning_rate": 2.7376012645722194e-05,
      "loss": 0.4862,
      "step": 2290
    },
    {
      "epoch": 1.3633669235328987,
      "grad_norm": 9.321807861328125,
      "learning_rate": 2.7277217941118355e-05,
      "loss": 0.4054,
      "step": 2300
    },
    {
      "epoch": 1.3692946058091287,
      "grad_norm": 6.245841979980469,
      "learning_rate": 2.7178423236514523e-05,
      "loss": 0.3897,
      "step": 2310
    },
    {
      "epoch": 1.3752222880853586,
      "grad_norm": 21.49048614501953,
      "learning_rate": 2.7079628531910688e-05,
      "loss": 0.3745,
      "step": 2320
    },
    {
      "epoch": 1.3811499703615886,
      "grad_norm": 23.670412063598633,
      "learning_rate": 2.6980833827306856e-05,
      "loss": 0.5331,
      "step": 2330
    },
    {
      "epoch": 1.3870776526378186,
      "grad_norm": 7.693662166595459,
      "learning_rate": 2.6882039122703023e-05,
      "loss": 0.502,
      "step": 2340
    },
    {
      "epoch": 1.3930053349140485,
      "grad_norm": 5.781563758850098,
      "learning_rate": 2.678324441809919e-05,
      "loss": 0.4203,
      "step": 2350
    },
    {
      "epoch": 1.3989330171902785,
      "grad_norm": 7.678214073181152,
      "learning_rate": 2.6684449713495356e-05,
      "loss": 0.393,
      "step": 2360
    },
    {
      "epoch": 1.4048606994665085,
      "grad_norm": 7.0040435791015625,
      "learning_rate": 2.6585655008891524e-05,
      "loss": 0.4468,
      "step": 2370
    },
    {
      "epoch": 1.4107883817427385,
      "grad_norm": 16.793054580688477,
      "learning_rate": 2.6486860304287692e-05,
      "loss": 0.4883,
      "step": 2380
    },
    {
      "epoch": 1.4167160640189687,
      "grad_norm": 14.5381441116333,
      "learning_rate": 2.638806559968386e-05,
      "loss": 0.3995,
      "step": 2390
    },
    {
      "epoch": 1.4226437462951986,
      "grad_norm": 33.188751220703125,
      "learning_rate": 2.6289270895080024e-05,
      "loss": 0.4006,
      "step": 2400
    },
    {
      "epoch": 1.4285714285714286,
      "grad_norm": 14.136757850646973,
      "learning_rate": 2.6190476190476192e-05,
      "loss": 0.4854,
      "step": 2410
    },
    {
      "epoch": 1.4344991108476586,
      "grad_norm": 46.16522979736328,
      "learning_rate": 2.609168148587236e-05,
      "loss": 0.3553,
      "step": 2420
    },
    {
      "epoch": 1.4404267931238885,
      "grad_norm": 43.665550231933594,
      "learning_rate": 2.5992886781268528e-05,
      "loss": 0.3931,
      "step": 2430
    },
    {
      "epoch": 1.4463544754001185,
      "grad_norm": 9.909360885620117,
      "learning_rate": 2.5894092076664696e-05,
      "loss": 0.488,
      "step": 2440
    },
    {
      "epoch": 1.4522821576763485,
      "grad_norm": 20.598766326904297,
      "learning_rate": 2.579529737206086e-05,
      "loss": 0.4275,
      "step": 2450
    },
    {
      "epoch": 1.4582098399525785,
      "grad_norm": 71.97425842285156,
      "learning_rate": 2.5696502667457022e-05,
      "loss": 0.4483,
      "step": 2460
    },
    {
      "epoch": 1.4641375222288087,
      "grad_norm": 7.717462539672852,
      "learning_rate": 2.559770796285319e-05,
      "loss": 0.4599,
      "step": 2470
    },
    {
      "epoch": 1.4700652045050386,
      "grad_norm": 11.084677696228027,
      "learning_rate": 2.5498913258249358e-05,
      "loss": 0.4729,
      "step": 2480
    },
    {
      "epoch": 1.4759928867812686,
      "grad_norm": 11.593571662902832,
      "learning_rate": 2.5400118553645525e-05,
      "loss": 0.5638,
      "step": 2490
    },
    {
      "epoch": 1.4819205690574986,
      "grad_norm": 41.46894454956055,
      "learning_rate": 2.530132384904169e-05,
      "loss": 0.5486,
      "step": 2500
    },
    {
      "epoch": 1.4878482513337286,
      "grad_norm": 14.49413013458252,
      "learning_rate": 2.5202529144437858e-05,
      "loss": 0.3943,
      "step": 2510
    },
    {
      "epoch": 1.4937759336099585,
      "grad_norm": 4.692091464996338,
      "learning_rate": 2.5103734439834026e-05,
      "loss": 0.377,
      "step": 2520
    },
    {
      "epoch": 1.4997036158861885,
      "grad_norm": 9.429450035095215,
      "learning_rate": 2.5004939735230194e-05,
      "loss": 0.3678,
      "step": 2530
    },
    {
      "epoch": 1.5056312981624185,
      "grad_norm": 771.1930541992188,
      "learning_rate": 2.490614503062636e-05,
      "loss": 0.4061,
      "step": 2540
    },
    {
      "epoch": 1.5115589804386484,
      "grad_norm": 9.893959045410156,
      "learning_rate": 2.4807350326022526e-05,
      "loss": 0.5649,
      "step": 2550
    },
    {
      "epoch": 1.5174866627148784,
      "grad_norm": 10.7116060256958,
      "learning_rate": 2.4708555621418694e-05,
      "loss": 0.5461,
      "step": 2560
    },
    {
      "epoch": 1.5234143449911084,
      "grad_norm": 8.141831398010254,
      "learning_rate": 2.4609760916814862e-05,
      "loss": 0.5039,
      "step": 2570
    },
    {
      "epoch": 1.5293420272673384,
      "grad_norm": 4.466522693634033,
      "learning_rate": 2.4510966212211027e-05,
      "loss": 0.4653,
      "step": 2580
    },
    {
      "epoch": 1.5352697095435683,
      "grad_norm": 14.824300765991211,
      "learning_rate": 2.441217150760719e-05,
      "loss": 0.5454,
      "step": 2590
    },
    {
      "epoch": 1.5411973918197983,
      "grad_norm": 8.726339340209961,
      "learning_rate": 2.431337680300336e-05,
      "loss": 0.5092,
      "step": 2600
    },
    {
      "epoch": 1.5471250740960285,
      "grad_norm": 25.562480926513672,
      "learning_rate": 2.4214582098399527e-05,
      "loss": 0.4882,
      "step": 2610
    },
    {
      "epoch": 1.5530527563722585,
      "grad_norm": 7.9986724853515625,
      "learning_rate": 2.4115787393795695e-05,
      "loss": 0.425,
      "step": 2620
    },
    {
      "epoch": 1.5589804386484885,
      "grad_norm": 72.96670532226562,
      "learning_rate": 2.401699268919186e-05,
      "loss": 0.441,
      "step": 2630
    },
    {
      "epoch": 1.5649081209247184,
      "grad_norm": 29.695537567138672,
      "learning_rate": 2.3918197984588027e-05,
      "loss": 0.3723,
      "step": 2640
    },
    {
      "epoch": 1.5708358032009484,
      "grad_norm": 32.98476791381836,
      "learning_rate": 2.3819403279984192e-05,
      "loss": 0.4276,
      "step": 2650
    },
    {
      "epoch": 1.5767634854771784,
      "grad_norm": 27.10540771484375,
      "learning_rate": 2.372060857538036e-05,
      "loss": 0.4489,
      "step": 2660
    },
    {
      "epoch": 1.5826911677534086,
      "grad_norm": 16.343748092651367,
      "learning_rate": 2.3621813870776528e-05,
      "loss": 0.4793,
      "step": 2670
    },
    {
      "epoch": 1.5886188500296385,
      "grad_norm": 96.24451446533203,
      "learning_rate": 2.3523019166172692e-05,
      "loss": 0.6039,
      "step": 2680
    },
    {
      "epoch": 1.5945465323058685,
      "grad_norm": 19.502498626708984,
      "learning_rate": 2.342422446156886e-05,
      "loss": 0.4297,
      "step": 2690
    },
    {
      "epoch": 1.6004742145820985,
      "grad_norm": 10.080013275146484,
      "learning_rate": 2.3325429756965028e-05,
      "loss": 0.3569,
      "step": 2700
    },
    {
      "epoch": 1.6064018968583285,
      "grad_norm": 25.78598403930664,
      "learning_rate": 2.3226635052361196e-05,
      "loss": 0.4256,
      "step": 2710
    },
    {
      "epoch": 1.6123295791345584,
      "grad_norm": 52.46147155761719,
      "learning_rate": 2.312784034775736e-05,
      "loss": 0.3781,
      "step": 2720
    },
    {
      "epoch": 1.6182572614107884,
      "grad_norm": 8.023353576660156,
      "learning_rate": 2.3029045643153525e-05,
      "loss": 0.3884,
      "step": 2730
    },
    {
      "epoch": 1.6241849436870184,
      "grad_norm": 13.239675521850586,
      "learning_rate": 2.2930250938549693e-05,
      "loss": 0.5072,
      "step": 2740
    },
    {
      "epoch": 1.6301126259632484,
      "grad_norm": 12.868340492248535,
      "learning_rate": 2.283145623394586e-05,
      "loss": 0.4619,
      "step": 2750
    },
    {
      "epoch": 1.6360403082394783,
      "grad_norm": 4.479226112365723,
      "learning_rate": 2.273266152934203e-05,
      "loss": 0.4502,
      "step": 2760
    },
    {
      "epoch": 1.6419679905157083,
      "grad_norm": 8.211589813232422,
      "learning_rate": 2.2633866824738194e-05,
      "loss": 0.4075,
      "step": 2770
    },
    {
      "epoch": 1.6478956727919383,
      "grad_norm": 161.88131713867188,
      "learning_rate": 2.253507212013436e-05,
      "loss": 0.4085,
      "step": 2780
    },
    {
      "epoch": 1.6538233550681682,
      "grad_norm": 27.124574661254883,
      "learning_rate": 2.243627741553053e-05,
      "loss": 0.4312,
      "step": 2790
    },
    {
      "epoch": 1.6597510373443982,
      "grad_norm": 22.21398162841797,
      "learning_rate": 2.2337482710926697e-05,
      "loss": 0.504,
      "step": 2800
    },
    {
      "epoch": 1.6656787196206282,
      "grad_norm": 19.872228622436523,
      "learning_rate": 2.2238688006322862e-05,
      "loss": 0.4947,
      "step": 2810
    },
    {
      "epoch": 1.6716064018968582,
      "grad_norm": 13.635519981384277,
      "learning_rate": 2.2139893301719027e-05,
      "loss": 0.3874,
      "step": 2820
    },
    {
      "epoch": 1.6775340841730884,
      "grad_norm": 16.54168128967285,
      "learning_rate": 2.2041098597115194e-05,
      "loss": 0.3952,
      "step": 2830
    },
    {
      "epoch": 1.6834617664493183,
      "grad_norm": 6.208676338195801,
      "learning_rate": 2.1942303892511362e-05,
      "loss": 0.3959,
      "step": 2840
    },
    {
      "epoch": 1.6893894487255483,
      "grad_norm": 8.01900577545166,
      "learning_rate": 2.184350918790753e-05,
      "loss": 0.479,
      "step": 2850
    },
    {
      "epoch": 1.6953171310017783,
      "grad_norm": 18.013652801513672,
      "learning_rate": 2.1744714483303695e-05,
      "loss": 0.509,
      "step": 2860
    },
    {
      "epoch": 1.7012448132780082,
      "grad_norm": 5.486523151397705,
      "learning_rate": 2.1645919778699863e-05,
      "loss": 0.3947,
      "step": 2870
    },
    {
      "epoch": 1.7071724955542384,
      "grad_norm": 12.441699981689453,
      "learning_rate": 2.154712507409603e-05,
      "loss": 0.4367,
      "step": 2880
    },
    {
      "epoch": 1.7131001778304684,
      "grad_norm": 28.861534118652344,
      "learning_rate": 2.1448330369492195e-05,
      "loss": 0.5846,
      "step": 2890
    },
    {
      "epoch": 1.7190278601066984,
      "grad_norm": 20.043113708496094,
      "learning_rate": 2.1349535664888363e-05,
      "loss": 0.3861,
      "step": 2900
    },
    {
      "epoch": 1.7249555423829284,
      "grad_norm": 16.412654876708984,
      "learning_rate": 2.1250740960284528e-05,
      "loss": 0.393,
      "step": 2910
    },
    {
      "epoch": 1.7308832246591583,
      "grad_norm": 20.56127166748047,
      "learning_rate": 2.1151946255680696e-05,
      "loss": 0.4428,
      "step": 2920
    },
    {
      "epoch": 1.7368109069353883,
      "grad_norm": 8.498906135559082,
      "learning_rate": 2.1053151551076864e-05,
      "loss": 0.3717,
      "step": 2930
    },
    {
      "epoch": 1.7427385892116183,
      "grad_norm": 16.137022018432617,
      "learning_rate": 2.095435684647303e-05,
      "loss": 0.4146,
      "step": 2940
    },
    {
      "epoch": 1.7486662714878483,
      "grad_norm": 11.034566879272461,
      "learning_rate": 2.0855562141869196e-05,
      "loss": 0.4798,
      "step": 2950
    },
    {
      "epoch": 1.7545939537640782,
      "grad_norm": 10.905204772949219,
      "learning_rate": 2.0756767437265364e-05,
      "loss": 0.3836,
      "step": 2960
    },
    {
      "epoch": 1.7605216360403082,
      "grad_norm": 21.20962905883789,
      "learning_rate": 2.065797273266153e-05,
      "loss": 0.3634,
      "step": 2970
    },
    {
      "epoch": 1.7664493183165382,
      "grad_norm": 8.364121437072754,
      "learning_rate": 2.0559178028057696e-05,
      "loss": 0.4191,
      "step": 2980
    },
    {
      "epoch": 1.7723770005927681,
      "grad_norm": 4.935337066650391,
      "learning_rate": 2.0460383323453864e-05,
      "loss": 0.4928,
      "step": 2990
    },
    {
      "epoch": 1.7783046828689981,
      "grad_norm": 16.50261878967285,
      "learning_rate": 2.036158861885003e-05,
      "loss": 0.4291,
      "step": 3000
    },
    {
      "epoch": 1.784232365145228,
      "grad_norm": 12.885981559753418,
      "learning_rate": 2.0262793914246197e-05,
      "loss": 0.4305,
      "step": 3010
    },
    {
      "epoch": 1.790160047421458,
      "grad_norm": 12.017006874084473,
      "learning_rate": 2.0163999209642365e-05,
      "loss": 0.5226,
      "step": 3020
    },
    {
      "epoch": 1.796087729697688,
      "grad_norm": 13.079777717590332,
      "learning_rate": 2.0065204505038533e-05,
      "loss": 0.4446,
      "step": 3030
    },
    {
      "epoch": 1.8020154119739182,
      "grad_norm": 18.528186798095703,
      "learning_rate": 1.9966409800434697e-05,
      "loss": 0.4277,
      "step": 3040
    },
    {
      "epoch": 1.8079430942501482,
      "grad_norm": 8.456416130065918,
      "learning_rate": 1.9867615095830862e-05,
      "loss": 0.4083,
      "step": 3050
    },
    {
      "epoch": 1.8138707765263782,
      "grad_norm": 11.28665542602539,
      "learning_rate": 1.976882039122703e-05,
      "loss": 0.3899,
      "step": 3060
    },
    {
      "epoch": 1.8197984588026082,
      "grad_norm": 9.363496780395508,
      "learning_rate": 1.9670025686623198e-05,
      "loss": 0.3488,
      "step": 3070
    },
    {
      "epoch": 1.8257261410788381,
      "grad_norm": 10.053664207458496,
      "learning_rate": 1.9571230982019366e-05,
      "loss": 0.3847,
      "step": 3080
    },
    {
      "epoch": 1.8316538233550683,
      "grad_norm": 156.98770141601562,
      "learning_rate": 1.947243627741553e-05,
      "loss": 0.4059,
      "step": 3090
    },
    {
      "epoch": 1.8375815056312983,
      "grad_norm": 22.405467987060547,
      "learning_rate": 1.9373641572811698e-05,
      "loss": 0.5201,
      "step": 3100
    },
    {
      "epoch": 1.8435091879075283,
      "grad_norm": 18.752473831176758,
      "learning_rate": 1.9274846868207866e-05,
      "loss": 0.3705,
      "step": 3110
    },
    {
      "epoch": 1.8494368701837582,
      "grad_norm": 12.559650421142578,
      "learning_rate": 1.9176052163604034e-05,
      "loss": 0.2973,
      "step": 3120
    },
    {
      "epoch": 1.8553645524599882,
      "grad_norm": 11.21545696258545,
      "learning_rate": 1.90772574590002e-05,
      "loss": 0.3934,
      "step": 3130
    },
    {
      "epoch": 1.8612922347362182,
      "grad_norm": 7.4538187980651855,
      "learning_rate": 1.8978462754396363e-05,
      "loss": 0.4178,
      "step": 3140
    },
    {
      "epoch": 1.8672199170124482,
      "grad_norm": 8.656839370727539,
      "learning_rate": 1.887966804979253e-05,
      "loss": 0.4839,
      "step": 3150
    },
    {
      "epoch": 1.8731475992886781,
      "grad_norm": 18.967723846435547,
      "learning_rate": 1.87808733451887e-05,
      "loss": 0.4043,
      "step": 3160
    },
    {
      "epoch": 1.879075281564908,
      "grad_norm": 2.652313709259033,
      "learning_rate": 1.8682078640584867e-05,
      "loss": 0.4065,
      "step": 3170
    },
    {
      "epoch": 1.885002963841138,
      "grad_norm": 4.124950885772705,
      "learning_rate": 1.858328393598103e-05,
      "loss": 0.3546,
      "step": 3180
    },
    {
      "epoch": 1.890930646117368,
      "grad_norm": 21.54342269897461,
      "learning_rate": 1.84844892313772e-05,
      "loss": 0.423,
      "step": 3190
    },
    {
      "epoch": 1.896858328393598,
      "grad_norm": 14.04715633392334,
      "learning_rate": 1.8385694526773367e-05,
      "loss": 0.4748,
      "step": 3200
    },
    {
      "epoch": 1.902786010669828,
      "grad_norm": 11.701040267944336,
      "learning_rate": 1.8286899822169532e-05,
      "loss": 0.3863,
      "step": 3210
    },
    {
      "epoch": 1.908713692946058,
      "grad_norm": 128.43197631835938,
      "learning_rate": 1.81881051175657e-05,
      "loss": 0.3644,
      "step": 3220
    },
    {
      "epoch": 1.914641375222288,
      "grad_norm": 11.4481201171875,
      "learning_rate": 1.8089310412961864e-05,
      "loss": 0.3906,
      "step": 3230
    },
    {
      "epoch": 1.920569057498518,
      "grad_norm": 16.677833557128906,
      "learning_rate": 1.7990515708358032e-05,
      "loss": 0.4422,
      "step": 3240
    },
    {
      "epoch": 1.9264967397747481,
      "grad_norm": 17.389015197753906,
      "learning_rate": 1.78917210037542e-05,
      "loss": 0.4421,
      "step": 3250
    },
    {
      "epoch": 1.932424422050978,
      "grad_norm": 13.845840454101562,
      "learning_rate": 1.7792926299150368e-05,
      "loss": 0.3419,
      "step": 3260
    },
    {
      "epoch": 1.938352104327208,
      "grad_norm": 37.173583984375,
      "learning_rate": 1.7694131594546533e-05,
      "loss": 0.4121,
      "step": 3270
    },
    {
      "epoch": 1.944279786603438,
      "grad_norm": 15.189359664916992,
      "learning_rate": 1.75953368899427e-05,
      "loss": 0.5034,
      "step": 3280
    },
    {
      "epoch": 1.950207468879668,
      "grad_norm": 5.964058876037598,
      "learning_rate": 1.7496542185338865e-05,
      "loss": 0.3884,
      "step": 3290
    },
    {
      "epoch": 1.9561351511558982,
      "grad_norm": 26.13176727294922,
      "learning_rate": 1.7397747480735033e-05,
      "loss": 0.3874,
      "step": 3300
    },
    {
      "epoch": 1.9620628334321282,
      "grad_norm": 14.379884719848633,
      "learning_rate": 1.72989527761312e-05,
      "loss": 0.4537,
      "step": 3310
    },
    {
      "epoch": 1.9679905157083581,
      "grad_norm": 13.412887573242188,
      "learning_rate": 1.7200158071527365e-05,
      "loss": 0.3937,
      "step": 3320
    },
    {
      "epoch": 1.9739181979845881,
      "grad_norm": 15.514087677001953,
      "learning_rate": 1.7101363366923533e-05,
      "loss": 0.47,
      "step": 3330
    },
    {
      "epoch": 1.979845880260818,
      "grad_norm": 106.9461898803711,
      "learning_rate": 1.70025686623197e-05,
      "loss": 0.4293,
      "step": 3340
    },
    {
      "epoch": 1.985773562537048,
      "grad_norm": 5.95479679107666,
      "learning_rate": 1.690377395771587e-05,
      "loss": 0.4253,
      "step": 3350
    },
    {
      "epoch": 1.991701244813278,
      "grad_norm": 16.082477569580078,
      "learning_rate": 1.6804979253112034e-05,
      "loss": 0.3308,
      "step": 3360
    },
    {
      "epoch": 1.997628927089508,
      "grad_norm": 5.0378594398498535,
      "learning_rate": 1.6706184548508198e-05,
      "loss": 0.3061,
      "step": 3370
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.8071449747998815,
      "eval_f1": 0.7688754663350507,
      "eval_loss": 0.4250943958759308,
      "eval_precision": 0.8228136882129278,
      "eval_recall": 0.7215738579526508,
      "eval_runtime": 13.3946,
      "eval_samples_per_second": 503.637,
      "eval_steps_per_second": 31.505,
      "step": 3374
    },
    {
      "epoch": 2.003556609365738,
      "grad_norm": 10.696497917175293,
      "learning_rate": 1.6607389843904366e-05,
      "loss": 0.4606,
      "step": 3380
    },
    {
      "epoch": 2.009484291641968,
      "grad_norm": 20.062734603881836,
      "learning_rate": 1.6508595139300534e-05,
      "loss": 0.3606,
      "step": 3390
    },
    {
      "epoch": 2.015411973918198,
      "grad_norm": 3.9526944160461426,
      "learning_rate": 1.6409800434696702e-05,
      "loss": 0.4194,
      "step": 3400
    },
    {
      "epoch": 2.021339656194428,
      "grad_norm": 6.853986740112305,
      "learning_rate": 1.6311005730092867e-05,
      "loss": 0.3746,
      "step": 3410
    },
    {
      "epoch": 2.027267338470658,
      "grad_norm": 8.084680557250977,
      "learning_rate": 1.6212211025489035e-05,
      "loss": 0.4322,
      "step": 3420
    },
    {
      "epoch": 2.033195020746888,
      "grad_norm": 35.949771881103516,
      "learning_rate": 1.6113416320885202e-05,
      "loss": 0.3746,
      "step": 3430
    },
    {
      "epoch": 2.039122703023118,
      "grad_norm": 4.075411319732666,
      "learning_rate": 1.6014621616281367e-05,
      "loss": 0.4088,
      "step": 3440
    },
    {
      "epoch": 2.045050385299348,
      "grad_norm": 7.754293441772461,
      "learning_rate": 1.5915826911677535e-05,
      "loss": 0.3845,
      "step": 3450
    },
    {
      "epoch": 2.0509780675755778,
      "grad_norm": 17.57101058959961,
      "learning_rate": 1.58170322070737e-05,
      "loss": 0.4313,
      "step": 3460
    },
    {
      "epoch": 2.0569057498518077,
      "grad_norm": 23.197431564331055,
      "learning_rate": 1.5718237502469867e-05,
      "loss": 0.3938,
      "step": 3470
    },
    {
      "epoch": 2.0628334321280377,
      "grad_norm": 3.9017465114593506,
      "learning_rate": 1.5619442797866035e-05,
      "loss": 0.3209,
      "step": 3480
    },
    {
      "epoch": 2.068761114404268,
      "grad_norm": 17.855857849121094,
      "learning_rate": 1.5520648093262203e-05,
      "loss": 0.3379,
      "step": 3490
    },
    {
      "epoch": 2.074688796680498,
      "grad_norm": 21.929885864257812,
      "learning_rate": 1.5421853388658368e-05,
      "loss": 0.4351,
      "step": 3500
    },
    {
      "epoch": 2.080616478956728,
      "grad_norm": 5.773061275482178,
      "learning_rate": 1.5323058684054536e-05,
      "loss": 0.4095,
      "step": 3510
    },
    {
      "epoch": 2.086544161232958,
      "grad_norm": 15.932950973510742,
      "learning_rate": 1.52242639794507e-05,
      "loss": 0.3746,
      "step": 3520
    },
    {
      "epoch": 2.092471843509188,
      "grad_norm": 9.97842788696289,
      "learning_rate": 1.5125469274846868e-05,
      "loss": 0.3772,
      "step": 3530
    },
    {
      "epoch": 2.098399525785418,
      "grad_norm": 4.5358500480651855,
      "learning_rate": 1.5026674570243034e-05,
      "loss": 0.3154,
      "step": 3540
    },
    {
      "epoch": 2.104327208061648,
      "grad_norm": 12.012264251708984,
      "learning_rate": 1.4927879865639202e-05,
      "loss": 0.2559,
      "step": 3550
    },
    {
      "epoch": 2.110254890337878,
      "grad_norm": 9.661776542663574,
      "learning_rate": 1.4829085161035369e-05,
      "loss": 0.3357,
      "step": 3560
    },
    {
      "epoch": 2.116182572614108,
      "grad_norm": 14.60178279876709,
      "learning_rate": 1.4730290456431537e-05,
      "loss": 0.3725,
      "step": 3570
    },
    {
      "epoch": 2.122110254890338,
      "grad_norm": 14.888642311096191,
      "learning_rate": 1.4631495751827703e-05,
      "loss": 0.3826,
      "step": 3580
    },
    {
      "epoch": 2.128037937166568,
      "grad_norm": 6.479857921600342,
      "learning_rate": 1.453270104722387e-05,
      "loss": 0.3291,
      "step": 3590
    },
    {
      "epoch": 2.133965619442798,
      "grad_norm": 17.80205535888672,
      "learning_rate": 1.4433906342620035e-05,
      "loss": 0.3923,
      "step": 3600
    },
    {
      "epoch": 2.139893301719028,
      "grad_norm": 14.73871898651123,
      "learning_rate": 1.4335111638016201e-05,
      "loss": 0.347,
      "step": 3610
    },
    {
      "epoch": 2.145820983995258,
      "grad_norm": 9.425739288330078,
      "learning_rate": 1.423631693341237e-05,
      "loss": 0.4134,
      "step": 3620
    },
    {
      "epoch": 2.1517486662714878,
      "grad_norm": 10.677457809448242,
      "learning_rate": 1.4137522228808536e-05,
      "loss": 0.3368,
      "step": 3630
    },
    {
      "epoch": 2.1576763485477177,
      "grad_norm": 9.370438575744629,
      "learning_rate": 1.4038727524204704e-05,
      "loss": 0.416,
      "step": 3640
    },
    {
      "epoch": 2.1636040308239477,
      "grad_norm": 11.79071044921875,
      "learning_rate": 1.393993281960087e-05,
      "loss": 0.3927,
      "step": 3650
    },
    {
      "epoch": 2.1695317131001777,
      "grad_norm": 12.982002258300781,
      "learning_rate": 1.3841138114997038e-05,
      "loss": 0.3928,
      "step": 3660
    },
    {
      "epoch": 2.1754593953764076,
      "grad_norm": 6.68670654296875,
      "learning_rate": 1.3742343410393204e-05,
      "loss": 0.3341,
      "step": 3670
    },
    {
      "epoch": 2.1813870776526376,
      "grad_norm": 10.00317668914795,
      "learning_rate": 1.3643548705789369e-05,
      "loss": 0.3528,
      "step": 3680
    },
    {
      "epoch": 2.187314759928868,
      "grad_norm": 4.50734806060791,
      "learning_rate": 1.3544754001185536e-05,
      "loss": 0.301,
      "step": 3690
    },
    {
      "epoch": 2.193242442205098,
      "grad_norm": 11.29925537109375,
      "learning_rate": 1.3445959296581703e-05,
      "loss": 0.3862,
      "step": 3700
    },
    {
      "epoch": 2.199170124481328,
      "grad_norm": 16.238370895385742,
      "learning_rate": 1.334716459197787e-05,
      "loss": 0.3682,
      "step": 3710
    },
    {
      "epoch": 2.205097806757558,
      "grad_norm": 11.391005516052246,
      "learning_rate": 1.3248369887374037e-05,
      "loss": 0.3438,
      "step": 3720
    },
    {
      "epoch": 2.211025489033788,
      "grad_norm": 33.706024169921875,
      "learning_rate": 1.3149575182770205e-05,
      "loss": 0.4381,
      "step": 3730
    },
    {
      "epoch": 2.216953171310018,
      "grad_norm": 9.201345443725586,
      "learning_rate": 1.3050780478166371e-05,
      "loss": 0.3738,
      "step": 3740
    },
    {
      "epoch": 2.222880853586248,
      "grad_norm": 42.99458312988281,
      "learning_rate": 1.2951985773562539e-05,
      "loss": 0.4208,
      "step": 3750
    },
    {
      "epoch": 2.228808535862478,
      "grad_norm": 2.641794204711914,
      "learning_rate": 1.2853191068958703e-05,
      "loss": 0.2934,
      "step": 3760
    },
    {
      "epoch": 2.234736218138708,
      "grad_norm": 5.9454545974731445,
      "learning_rate": 1.275439636435487e-05,
      "loss": 0.3984,
      "step": 3770
    },
    {
      "epoch": 2.240663900414938,
      "grad_norm": 30.261106491088867,
      "learning_rate": 1.2655601659751038e-05,
      "loss": 0.3751,
      "step": 3780
    },
    {
      "epoch": 2.2465915826911678,
      "grad_norm": 17.346529006958008,
      "learning_rate": 1.2556806955147204e-05,
      "loss": 0.3412,
      "step": 3790
    },
    {
      "epoch": 2.2525192649673977,
      "grad_norm": 38.81993103027344,
      "learning_rate": 1.2458012250543372e-05,
      "loss": 0.4274,
      "step": 3800
    },
    {
      "epoch": 2.2584469472436277,
      "grad_norm": 9.575560569763184,
      "learning_rate": 1.2359217545939538e-05,
      "loss": 0.4116,
      "step": 3810
    },
    {
      "epoch": 2.2643746295198577,
      "grad_norm": 12.706289291381836,
      "learning_rate": 1.2260422841335704e-05,
      "loss": 0.4356,
      "step": 3820
    },
    {
      "epoch": 2.2703023117960877,
      "grad_norm": 14.31711483001709,
      "learning_rate": 1.2161628136731872e-05,
      "loss": 0.3298,
      "step": 3830
    },
    {
      "epoch": 2.2762299940723176,
      "grad_norm": 20.180713653564453,
      "learning_rate": 1.2062833432128038e-05,
      "loss": 0.4103,
      "step": 3840
    },
    {
      "epoch": 2.2821576763485476,
      "grad_norm": 43.02312088012695,
      "learning_rate": 1.1964038727524206e-05,
      "loss": 0.4038,
      "step": 3850
    },
    {
      "epoch": 2.2880853586247776,
      "grad_norm": 13.239827156066895,
      "learning_rate": 1.1865244022920371e-05,
      "loss": 0.3272,
      "step": 3860
    },
    {
      "epoch": 2.2940130409010075,
      "grad_norm": 13.815536499023438,
      "learning_rate": 1.1766449318316539e-05,
      "loss": 0.3307,
      "step": 3870
    },
    {
      "epoch": 2.2999407231772375,
      "grad_norm": 31.186370849609375,
      "learning_rate": 1.1667654613712705e-05,
      "loss": 0.3749,
      "step": 3880
    },
    {
      "epoch": 2.3058684054534675,
      "grad_norm": 2.952834129333496,
      "learning_rate": 1.1568859909108873e-05,
      "loss": 0.4338,
      "step": 3890
    },
    {
      "epoch": 2.3117960877296975,
      "grad_norm": 39.633365631103516,
      "learning_rate": 1.147006520450504e-05,
      "loss": 0.3161,
      "step": 3900
    },
    {
      "epoch": 2.3177237700059274,
      "grad_norm": 5.159843921661377,
      "learning_rate": 1.1371270499901205e-05,
      "loss": 0.3748,
      "step": 3910
    },
    {
      "epoch": 2.323651452282158,
      "grad_norm": 3.8396642208099365,
      "learning_rate": 1.1272475795297373e-05,
      "loss": 0.3022,
      "step": 3920
    },
    {
      "epoch": 2.329579134558388,
      "grad_norm": 16.763219833374023,
      "learning_rate": 1.117368109069354e-05,
      "loss": 0.4126,
      "step": 3930
    },
    {
      "epoch": 2.335506816834618,
      "grad_norm": 16.62257194519043,
      "learning_rate": 1.1074886386089706e-05,
      "loss": 0.2794,
      "step": 3940
    },
    {
      "epoch": 2.3414344991108478,
      "grad_norm": 59.22440719604492,
      "learning_rate": 1.0976091681485872e-05,
      "loss": 0.3518,
      "step": 3950
    },
    {
      "epoch": 2.3473621813870778,
      "grad_norm": 25.367137908935547,
      "learning_rate": 1.087729697688204e-05,
      "loss": 0.4464,
      "step": 3960
    },
    {
      "epoch": 2.3532898636633077,
      "grad_norm": 15.791176795959473,
      "learning_rate": 1.0778502272278206e-05,
      "loss": 0.4344,
      "step": 3970
    },
    {
      "epoch": 2.3592175459395377,
      "grad_norm": 14.19846248626709,
      "learning_rate": 1.0679707567674373e-05,
      "loss": 0.4047,
      "step": 3980
    },
    {
      "epoch": 2.3651452282157677,
      "grad_norm": 6.5274858474731445,
      "learning_rate": 1.058091286307054e-05,
      "loss": 0.3987,
      "step": 3990
    },
    {
      "epoch": 2.3710729104919976,
      "grad_norm": 5.54861307144165,
      "learning_rate": 1.0482118158466707e-05,
      "loss": 0.3418,
      "step": 4000
    },
    {
      "epoch": 2.3770005927682276,
      "grad_norm": 16.294950485229492,
      "learning_rate": 1.0383323453862875e-05,
      "loss": 0.3424,
      "step": 4010
    },
    {
      "epoch": 2.3829282750444576,
      "grad_norm": 9.854000091552734,
      "learning_rate": 1.0284528749259039e-05,
      "loss": 0.4192,
      "step": 4020
    },
    {
      "epoch": 2.3888559573206876,
      "grad_norm": 163.8497772216797,
      "learning_rate": 1.0185734044655207e-05,
      "loss": 0.4127,
      "step": 4030
    },
    {
      "epoch": 2.3947836395969175,
      "grad_norm": 6.262341499328613,
      "learning_rate": 1.0086939340051373e-05,
      "loss": 0.3857,
      "step": 4040
    },
    {
      "epoch": 2.4007113218731475,
      "grad_norm": 6.851419925689697,
      "learning_rate": 9.988144635447541e-06,
      "loss": 0.3674,
      "step": 4050
    },
    {
      "epoch": 2.4066390041493775,
      "grad_norm": 38.52494812011719,
      "learning_rate": 9.889349930843707e-06,
      "loss": 0.374,
      "step": 4060
    },
    {
      "epoch": 2.4125666864256075,
      "grad_norm": 9.862377166748047,
      "learning_rate": 9.790555226239874e-06,
      "loss": 0.3233,
      "step": 4070
    },
    {
      "epoch": 2.4184943687018374,
      "grad_norm": 6.789559841156006,
      "learning_rate": 9.691760521636042e-06,
      "loss": 0.3409,
      "step": 4080
    },
    {
      "epoch": 2.4244220509780674,
      "grad_norm": 26.37495231628418,
      "learning_rate": 9.592965817032208e-06,
      "loss": 0.3328,
      "step": 4090
    },
    {
      "epoch": 2.430349733254298,
      "grad_norm": 36.91740036010742,
      "learning_rate": 9.494171112428374e-06,
      "loss": 0.3649,
      "step": 4100
    },
    {
      "epoch": 2.436277415530528,
      "grad_norm": 9.01511287689209,
      "learning_rate": 9.39537640782454e-06,
      "loss": 0.4495,
      "step": 4110
    },
    {
      "epoch": 2.4422050978067578,
      "grad_norm": 26.077003479003906,
      "learning_rate": 9.296581703220708e-06,
      "loss": 0.2732,
      "step": 4120
    },
    {
      "epoch": 2.4481327800829877,
      "grad_norm": 31.57008171081543,
      "learning_rate": 9.197786998616875e-06,
      "loss": 0.3523,
      "step": 4130
    },
    {
      "epoch": 2.4540604623592177,
      "grad_norm": 3.593698740005493,
      "learning_rate": 9.09899229401304e-06,
      "loss": 0.3791,
      "step": 4140
    },
    {
      "epoch": 2.4599881446354477,
      "grad_norm": 37.96546173095703,
      "learning_rate": 9.000197589409209e-06,
      "loss": 0.334,
      "step": 4150
    },
    {
      "epoch": 2.4659158269116777,
      "grad_norm": 18.886323928833008,
      "learning_rate": 8.901402884805375e-06,
      "loss": 0.3698,
      "step": 4160
    },
    {
      "epoch": 2.4718435091879076,
      "grad_norm": 6.217711925506592,
      "learning_rate": 8.802608180201543e-06,
      "loss": 0.3339,
      "step": 4170
    },
    {
      "epoch": 2.4777711914641376,
      "grad_norm": 58.29039001464844,
      "learning_rate": 8.703813475597707e-06,
      "loss": 0.3766,
      "step": 4180
    },
    {
      "epoch": 2.4836988737403676,
      "grad_norm": 11.24885082244873,
      "learning_rate": 8.605018770993875e-06,
      "loss": 0.4108,
      "step": 4190
    },
    {
      "epoch": 2.4896265560165975,
      "grad_norm": 20.17118263244629,
      "learning_rate": 8.506224066390042e-06,
      "loss": 0.33,
      "step": 4200
    },
    {
      "epoch": 2.4955542382928275,
      "grad_norm": 186.5379638671875,
      "learning_rate": 8.40742936178621e-06,
      "loss": 0.34,
      "step": 4210
    },
    {
      "epoch": 2.5014819205690575,
      "grad_norm": 8.806441307067871,
      "learning_rate": 8.308634657182376e-06,
      "loss": 0.4029,
      "step": 4220
    },
    {
      "epoch": 2.5074096028452875,
      "grad_norm": 15.99166202545166,
      "learning_rate": 8.209839952578542e-06,
      "loss": 0.4421,
      "step": 4230
    },
    {
      "epoch": 2.5133372851215174,
      "grad_norm": 9.597944259643555,
      "learning_rate": 8.11104524797471e-06,
      "loss": 0.3328,
      "step": 4240
    },
    {
      "epoch": 2.5192649673977474,
      "grad_norm": 42.531150817871094,
      "learning_rate": 8.012250543370876e-06,
      "loss": 0.3236,
      "step": 4250
    },
    {
      "epoch": 2.5251926496739774,
      "grad_norm": 14.334115028381348,
      "learning_rate": 7.913455838767042e-06,
      "loss": 0.363,
      "step": 4260
    },
    {
      "epoch": 2.5311203319502074,
      "grad_norm": 14.871476173400879,
      "learning_rate": 7.814661134163209e-06,
      "loss": 0.4744,
      "step": 4270
    },
    {
      "epoch": 2.5370480142264373,
      "grad_norm": 8.594168663024902,
      "learning_rate": 7.715866429559377e-06,
      "loss": 0.4856,
      "step": 4280
    },
    {
      "epoch": 2.5429756965026673,
      "grad_norm": 23.842939376831055,
      "learning_rate": 7.617071724955542e-06,
      "loss": 0.3655,
      "step": 4290
    },
    {
      "epoch": 2.5489033787788973,
      "grad_norm": 11.9024076461792,
      "learning_rate": 7.518277020351709e-06,
      "loss": 0.3141,
      "step": 4300
    },
    {
      "epoch": 2.5548310610551273,
      "grad_norm": 28.025917053222656,
      "learning_rate": 7.419482315747876e-06,
      "loss": 0.4346,
      "step": 4310
    },
    {
      "epoch": 2.5607587433313572,
      "grad_norm": 21.405149459838867,
      "learning_rate": 7.320687611144043e-06,
      "loss": 0.4584,
      "step": 4320
    },
    {
      "epoch": 2.566686425607587,
      "grad_norm": 88.81070709228516,
      "learning_rate": 7.221892906540209e-06,
      "loss": 0.3464,
      "step": 4330
    },
    {
      "epoch": 2.572614107883817,
      "grad_norm": 9.348417282104492,
      "learning_rate": 7.1230982019363765e-06,
      "loss": 0.3239,
      "step": 4340
    },
    {
      "epoch": 2.5785417901600476,
      "grad_norm": 17.077791213989258,
      "learning_rate": 7.0243034973325436e-06,
      "loss": 0.4587,
      "step": 4350
    },
    {
      "epoch": 2.5844694724362776,
      "grad_norm": 12.10830307006836,
      "learning_rate": 6.925508792728711e-06,
      "loss": 0.3747,
      "step": 4360
    },
    {
      "epoch": 2.5903971547125075,
      "grad_norm": 14.850526809692383,
      "learning_rate": 6.826714088124876e-06,
      "loss": 0.3478,
      "step": 4370
    },
    {
      "epoch": 2.5963248369887375,
      "grad_norm": 6.169094562530518,
      "learning_rate": 6.727919383521043e-06,
      "loss": 0.3029,
      "step": 4380
    },
    {
      "epoch": 2.6022525192649675,
      "grad_norm": 10.777155876159668,
      "learning_rate": 6.62912467891721e-06,
      "loss": 0.2763,
      "step": 4390
    },
    {
      "epoch": 2.6081802015411975,
      "grad_norm": 23.142183303833008,
      "learning_rate": 6.530329974313377e-06,
      "loss": 0.4054,
      "step": 4400
    },
    {
      "epoch": 2.6141078838174274,
      "grad_norm": 9.002660751342773,
      "learning_rate": 6.4315352697095435e-06,
      "loss": 0.3324,
      "step": 4410
    },
    {
      "epoch": 2.6200355660936574,
      "grad_norm": 82.47757720947266,
      "learning_rate": 6.332740565105711e-06,
      "loss": 0.3523,
      "step": 4420
    },
    {
      "epoch": 2.6259632483698874,
      "grad_norm": 5.4543538093566895,
      "learning_rate": 6.233945860501878e-06,
      "loss": 0.3473,
      "step": 4430
    },
    {
      "epoch": 2.6318909306461173,
      "grad_norm": 20.041963577270508,
      "learning_rate": 6.135151155898044e-06,
      "loss": 0.3367,
      "step": 4440
    },
    {
      "epoch": 2.6378186129223473,
      "grad_norm": 38.609867095947266,
      "learning_rate": 6.036356451294211e-06,
      "loss": 0.3135,
      "step": 4450
    },
    {
      "epoch": 2.6437462951985773,
      "grad_norm": 12.463240623474121,
      "learning_rate": 5.937561746690377e-06,
      "loss": 0.3609,
      "step": 4460
    },
    {
      "epoch": 2.6496739774748073,
      "grad_norm": 22.34029769897461,
      "learning_rate": 5.838767042086544e-06,
      "loss": 0.386,
      "step": 4470
    },
    {
      "epoch": 2.6556016597510372,
      "grad_norm": 194.08314514160156,
      "learning_rate": 5.7399723374827105e-06,
      "loss": 0.404,
      "step": 4480
    },
    {
      "epoch": 2.661529342027267,
      "grad_norm": 26.393346786499023,
      "learning_rate": 5.641177632878878e-06,
      "loss": 0.3664,
      "step": 4490
    },
    {
      "epoch": 2.667457024303497,
      "grad_norm": 46.99919128417969,
      "learning_rate": 5.542382928275045e-06,
      "loss": 0.3015,
      "step": 4500
    },
    {
      "epoch": 2.6733847065797276,
      "grad_norm": 23.56463623046875,
      "learning_rate": 5.443588223671212e-06,
      "loss": 0.3687,
      "step": 4510
    },
    {
      "epoch": 2.6793123888559576,
      "grad_norm": 22.895549774169922,
      "learning_rate": 5.344793519067378e-06,
      "loss": 0.3651,
      "step": 4520
    },
    {
      "epoch": 2.6852400711321875,
      "grad_norm": 24.691328048706055,
      "learning_rate": 5.245998814463545e-06,
      "loss": 0.406,
      "step": 4530
    },
    {
      "epoch": 2.6911677534084175,
      "grad_norm": 5.026595592498779,
      "learning_rate": 5.147204109859711e-06,
      "loss": 0.385,
      "step": 4540
    },
    {
      "epoch": 2.6970954356846475,
      "grad_norm": 4.141049861907959,
      "learning_rate": 5.0484094052558784e-06,
      "loss": 0.3157,
      "step": 4550
    },
    {
      "epoch": 2.7030231179608775,
      "grad_norm": 38.45936584472656,
      "learning_rate": 4.949614700652045e-06,
      "loss": 0.3677,
      "step": 4560
    },
    {
      "epoch": 2.7089508002371074,
      "grad_norm": 53.229576110839844,
      "learning_rate": 4.850819996048212e-06,
      "loss": 0.3296,
      "step": 4570
    },
    {
      "epoch": 2.7148784825133374,
      "grad_norm": 12.920677185058594,
      "learning_rate": 4.752025291444379e-06,
      "loss": 0.2627,
      "step": 4580
    },
    {
      "epoch": 2.7208061647895674,
      "grad_norm": 20.540285110473633,
      "learning_rate": 4.653230586840546e-06,
      "loss": 0.3958,
      "step": 4590
    },
    {
      "epoch": 2.7267338470657974,
      "grad_norm": 229.41458129882812,
      "learning_rate": 4.554435882236712e-06,
      "loss": 0.4271,
      "step": 4600
    },
    {
      "epoch": 2.7326615293420273,
      "grad_norm": 17.243295669555664,
      "learning_rate": 4.455641177632879e-06,
      "loss": 0.3226,
      "step": 4610
    },
    {
      "epoch": 2.7385892116182573,
      "grad_norm": 20.252227783203125,
      "learning_rate": 4.3568464730290455e-06,
      "loss": 0.4157,
      "step": 4620
    },
    {
      "epoch": 2.7445168938944873,
      "grad_norm": 7.780714511871338,
      "learning_rate": 4.2580517684252125e-06,
      "loss": 0.4575,
      "step": 4630
    },
    {
      "epoch": 2.7504445761707172,
      "grad_norm": 17.713573455810547,
      "learning_rate": 4.159257063821379e-06,
      "loss": 0.3604,
      "step": 4640
    },
    {
      "epoch": 2.756372258446947,
      "grad_norm": 12.684194564819336,
      "learning_rate": 4.060462359217546e-06,
      "loss": 0.3853,
      "step": 4650
    },
    {
      "epoch": 2.762299940723177,
      "grad_norm": 11.402897834777832,
      "learning_rate": 3.961667654613713e-06,
      "loss": 0.2977,
      "step": 4660
    },
    {
      "epoch": 2.768227622999407,
      "grad_norm": 22.764951705932617,
      "learning_rate": 3.86287295000988e-06,
      "loss": 0.3543,
      "step": 4670
    },
    {
      "epoch": 2.774155305275637,
      "grad_norm": 3.876777172088623,
      "learning_rate": 3.7640782454060463e-06,
      "loss": 0.3215,
      "step": 4680
    },
    {
      "epoch": 2.780082987551867,
      "grad_norm": 21.799381256103516,
      "learning_rate": 3.6652835408022133e-06,
      "loss": 0.3883,
      "step": 4690
    },
    {
      "epoch": 2.786010669828097,
      "grad_norm": 15.203004837036133,
      "learning_rate": 3.5664888361983796e-06,
      "loss": 0.4192,
      "step": 4700
    },
    {
      "epoch": 2.791938352104327,
      "grad_norm": 85.35291290283203,
      "learning_rate": 3.4676941315945467e-06,
      "loss": 0.3506,
      "step": 4710
    },
    {
      "epoch": 2.797866034380557,
      "grad_norm": 27.268320083618164,
      "learning_rate": 3.3688994269907133e-06,
      "loss": 0.3338,
      "step": 4720
    },
    {
      "epoch": 2.803793716656787,
      "grad_norm": 44.047523498535156,
      "learning_rate": 3.2701047223868804e-06,
      "loss": 0.3382,
      "step": 4730
    },
    {
      "epoch": 2.809721398933017,
      "grad_norm": 21.36154556274414,
      "learning_rate": 3.1713100177830466e-06,
      "loss": 0.2626,
      "step": 4740
    },
    {
      "epoch": 2.815649081209247,
      "grad_norm": 7.941445350646973,
      "learning_rate": 3.0725153131792137e-06,
      "loss": 0.3965,
      "step": 4750
    },
    {
      "epoch": 2.821576763485477,
      "grad_norm": 18.896150588989258,
      "learning_rate": 2.9737206085753804e-06,
      "loss": 0.2914,
      "step": 4760
    },
    {
      "epoch": 2.827504445761707,
      "grad_norm": 12.08133602142334,
      "learning_rate": 2.874925903971547e-06,
      "loss": 0.3707,
      "step": 4770
    },
    {
      "epoch": 2.8334321280379373,
      "grad_norm": 5.984476566314697,
      "learning_rate": 2.7761311993677137e-06,
      "loss": 0.2787,
      "step": 4780
    },
    {
      "epoch": 2.8393598103141673,
      "grad_norm": 16.540746688842773,
      "learning_rate": 2.6773364947638808e-06,
      "loss": 0.2881,
      "step": 4790
    },
    {
      "epoch": 2.8452874925903973,
      "grad_norm": 10.331686973571777,
      "learning_rate": 2.5785417901600474e-06,
      "loss": 0.3284,
      "step": 4800
    },
    {
      "epoch": 2.8512151748666272,
      "grad_norm": 32.8717041015625,
      "learning_rate": 2.479747085556214e-06,
      "loss": 0.4498,
      "step": 4810
    },
    {
      "epoch": 2.857142857142857,
      "grad_norm": 12.270064353942871,
      "learning_rate": 2.3809523809523808e-06,
      "loss": 0.3313,
      "step": 4820
    },
    {
      "epoch": 2.863070539419087,
      "grad_norm": 6.521349906921387,
      "learning_rate": 2.282157676348548e-06,
      "loss": 0.3754,
      "step": 4830
    },
    {
      "epoch": 2.868998221695317,
      "grad_norm": 94.0171890258789,
      "learning_rate": 2.1833629717447145e-06,
      "loss": 0.3634,
      "step": 4840
    },
    {
      "epoch": 2.874925903971547,
      "grad_norm": 8.356252670288086,
      "learning_rate": 2.084568267140881e-06,
      "loss": 0.357,
      "step": 4850
    },
    {
      "epoch": 2.880853586247777,
      "grad_norm": 11.626572608947754,
      "learning_rate": 1.985773562537048e-06,
      "loss": 0.3223,
      "step": 4860
    },
    {
      "epoch": 2.886781268524007,
      "grad_norm": 4.0129923820495605,
      "learning_rate": 1.8869788579332147e-06,
      "loss": 0.3236,
      "step": 4870
    },
    {
      "epoch": 2.892708950800237,
      "grad_norm": 8.26845645904541,
      "learning_rate": 1.7881841533293816e-06,
      "loss": 0.3225,
      "step": 4880
    },
    {
      "epoch": 2.898636633076467,
      "grad_norm": 12.8668794631958,
      "learning_rate": 1.6893894487255482e-06,
      "loss": 0.3997,
      "step": 4890
    },
    {
      "epoch": 2.904564315352697,
      "grad_norm": 13.761783599853516,
      "learning_rate": 1.590594744121715e-06,
      "loss": 0.3469,
      "step": 4900
    },
    {
      "epoch": 2.910491997628927,
      "grad_norm": 5.733354568481445,
      "learning_rate": 1.491800039517882e-06,
      "loss": 0.2739,
      "step": 4910
    },
    {
      "epoch": 2.916419679905157,
      "grad_norm": 21.526737213134766,
      "learning_rate": 1.3930053349140488e-06,
      "loss": 0.2985,
      "step": 4920
    },
    {
      "epoch": 2.9223473621813874,
      "grad_norm": 24.89443588256836,
      "learning_rate": 1.2942106303102155e-06,
      "loss": 0.2968,
      "step": 4930
    },
    {
      "epoch": 2.9282750444576173,
      "grad_norm": 18.114564895629883,
      "learning_rate": 1.1954159257063822e-06,
      "loss": 0.3077,
      "step": 4940
    },
    {
      "epoch": 2.9342027267338473,
      "grad_norm": 7.298433780670166,
      "learning_rate": 1.0966212211025488e-06,
      "loss": 0.3119,
      "step": 4950
    },
    {
      "epoch": 2.9401304090100773,
      "grad_norm": 14.937835693359375,
      "learning_rate": 9.978265164987157e-07,
      "loss": 0.2945,
      "step": 4960
    },
    {
      "epoch": 2.9460580912863072,
      "grad_norm": 11.313472747802734,
      "learning_rate": 8.990318118948824e-07,
      "loss": 0.3006,
      "step": 4970
    },
    {
      "epoch": 2.951985773562537,
      "grad_norm": 7.192227363586426,
      "learning_rate": 8.002371072910492e-07,
      "loss": 0.3627,
      "step": 4980
    },
    {
      "epoch": 2.957913455838767,
      "grad_norm": 17.877391815185547,
      "learning_rate": 7.01442402687216e-07,
      "loss": 0.3917,
      "step": 4990
    },
    {
      "epoch": 2.963841138114997,
      "grad_norm": 19.31447982788086,
      "learning_rate": 6.026476980833827e-07,
      "loss": 0.335,
      "step": 5000
    },
    {
      "epoch": 2.969768820391227,
      "grad_norm": 14.860166549682617,
      "learning_rate": 5.038529934795495e-07,
      "loss": 0.3017,
      "step": 5010
    },
    {
      "epoch": 2.975696502667457,
      "grad_norm": 19.493515014648438,
      "learning_rate": 4.0505828887571633e-07,
      "loss": 0.3317,
      "step": 5020
    },
    {
      "epoch": 2.981624184943687,
      "grad_norm": 12.655961990356445,
      "learning_rate": 3.0626358427188304e-07,
      "loss": 0.3756,
      "step": 5030
    },
    {
      "epoch": 2.987551867219917,
      "grad_norm": 16.331369400024414,
      "learning_rate": 2.074688796680498e-07,
      "loss": 0.3367,
      "step": 5040
    },
    {
      "epoch": 2.993479549496147,
      "grad_norm": 47.0096435546875,
      "learning_rate": 1.0867417506421657e-07,
      "loss": 0.4039,
      "step": 5050
    },
    {
      "epoch": 2.999407231772377,
      "grad_norm": 25.627880096435547,
      "learning_rate": 9.879470460383324e-09,
      "loss": 0.3401,
      "step": 5060
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.8274533056626149,
      "eval_f1": 0.8060646451182939,
      "eval_loss": 0.3948293626308441,
      "eval_precision": 0.8055278055278056,
      "eval_recall": 0.8066022007335779,
      "eval_runtime": 13.7762,
      "eval_samples_per_second": 489.683,
      "eval_steps_per_second": 30.632,
      "step": 5061
    }
  ],
  "logging_steps": 10,
  "max_steps": 5061,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 483631859842560.0,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
