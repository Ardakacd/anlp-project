{
  "best_metric": 0.4250943958759308,
  "best_model_checkpoint": "c:\\Users\\chuvi\\Code\\SUCode\\ANLP\\anlp-project\\anlp-project\\AutoDep_Master\\results\\text\\albert-base-v2\\checkpoint-3374",
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 3374,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.005927682276229994,
      "grad_norm": 10.161124229431152,
      "learning_rate": 4.990120529539617e-05,
      "loss": 0.6908,
      "step": 10
    },
    {
      "epoch": 0.011855364552459988,
      "grad_norm": 13.26170825958252,
      "learning_rate": 4.980241059079234e-05,
      "loss": 0.6408,
      "step": 20
    },
    {
      "epoch": 0.01778304682868998,
      "grad_norm": 15.047383308410645,
      "learning_rate": 4.9703615886188506e-05,
      "loss": 0.6749,
      "step": 30
    },
    {
      "epoch": 0.023710729104919975,
      "grad_norm": 11.168242454528809,
      "learning_rate": 4.9604821181584674e-05,
      "loss": 0.6578,
      "step": 40
    },
    {
      "epoch": 0.02963841138114997,
      "grad_norm": 16.361614227294922,
      "learning_rate": 4.9506026476980835e-05,
      "loss": 0.6153,
      "step": 50
    },
    {
      "epoch": 0.03556609365737996,
      "grad_norm": 10.466825485229492,
      "learning_rate": 4.9407231772377e-05,
      "loss": 0.6618,
      "step": 60
    },
    {
      "epoch": 0.04149377593360996,
      "grad_norm": 20.217893600463867,
      "learning_rate": 4.930843706777317e-05,
      "loss": 0.6304,
      "step": 70
    },
    {
      "epoch": 0.04742145820983995,
      "grad_norm": 13.51468563079834,
      "learning_rate": 4.920964236316933e-05,
      "loss": 0.5592,
      "step": 80
    },
    {
      "epoch": 0.053349140486069944,
      "grad_norm": 9.34771728515625,
      "learning_rate": 4.91108476585655e-05,
      "loss": 0.5899,
      "step": 90
    },
    {
      "epoch": 0.05927682276229994,
      "grad_norm": 42.417335510253906,
      "learning_rate": 4.901205295396167e-05,
      "loss": 0.6186,
      "step": 100
    },
    {
      "epoch": 0.06520450503852994,
      "grad_norm": 15.10250473022461,
      "learning_rate": 4.8913258249357836e-05,
      "loss": 0.6338,
      "step": 110
    },
    {
      "epoch": 0.07113218731475993,
      "grad_norm": 250.51513671875,
      "learning_rate": 4.8814463544754004e-05,
      "loss": 0.6145,
      "step": 120
    },
    {
      "epoch": 0.07705986959098993,
      "grad_norm": 42.902496337890625,
      "learning_rate": 4.871566884015017e-05,
      "loss": 0.6367,
      "step": 130
    },
    {
      "epoch": 0.08298755186721991,
      "grad_norm": 10.545756340026855,
      "learning_rate": 4.861687413554633e-05,
      "loss": 0.6419,
      "step": 140
    },
    {
      "epoch": 0.08891523414344991,
      "grad_norm": 26.169902801513672,
      "learning_rate": 4.85180794309425e-05,
      "loss": 0.5338,
      "step": 150
    },
    {
      "epoch": 0.0948429164196799,
      "grad_norm": 19.757549285888672,
      "learning_rate": 4.841928472633867e-05,
      "loss": 0.6635,
      "step": 160
    },
    {
      "epoch": 0.1007705986959099,
      "grad_norm": 15.494487762451172,
      "learning_rate": 4.832049002173484e-05,
      "loss": 0.5874,
      "step": 170
    },
    {
      "epoch": 0.10669828097213989,
      "grad_norm": 16.69301986694336,
      "learning_rate": 4.8221695317131005e-05,
      "loss": 0.5775,
      "step": 180
    },
    {
      "epoch": 0.11262596324836989,
      "grad_norm": 7.297093391418457,
      "learning_rate": 4.812290061252717e-05,
      "loss": 0.5198,
      "step": 190
    },
    {
      "epoch": 0.11855364552459988,
      "grad_norm": 33.785987854003906,
      "learning_rate": 4.802410590792334e-05,
      "loss": 0.6286,
      "step": 200
    },
    {
      "epoch": 0.12448132780082988,
      "grad_norm": 9.313821792602539,
      "learning_rate": 4.792531120331951e-05,
      "loss": 0.6568,
      "step": 210
    },
    {
      "epoch": 0.13040901007705988,
      "grad_norm": 11.622394561767578,
      "learning_rate": 4.7826516498715677e-05,
      "loss": 0.6086,
      "step": 220
    },
    {
      "epoch": 0.13633669235328985,
      "grad_norm": 15.57612419128418,
      "learning_rate": 4.772772179411184e-05,
      "loss": 0.576,
      "step": 230
    },
    {
      "epoch": 0.14226437462951985,
      "grad_norm": 6.794140815734863,
      "learning_rate": 4.7628927089508e-05,
      "loss": 0.5607,
      "step": 240
    },
    {
      "epoch": 0.14819205690574985,
      "grad_norm": 14.009489059448242,
      "learning_rate": 4.753013238490417e-05,
      "loss": 0.5411,
      "step": 250
    },
    {
      "epoch": 0.15411973918197985,
      "grad_norm": 11.607579231262207,
      "learning_rate": 4.7431337680300335e-05,
      "loss": 0.5558,
      "step": 260
    },
    {
      "epoch": 0.16004742145820983,
      "grad_norm": 10.619332313537598,
      "learning_rate": 4.73325429756965e-05,
      "loss": 0.6315,
      "step": 270
    },
    {
      "epoch": 0.16597510373443983,
      "grad_norm": 5.460982322692871,
      "learning_rate": 4.723374827109267e-05,
      "loss": 0.6012,
      "step": 280
    },
    {
      "epoch": 0.17190278601066983,
      "grad_norm": 8.422821998596191,
      "learning_rate": 4.713495356648884e-05,
      "loss": 0.5598,
      "step": 290
    },
    {
      "epoch": 0.17783046828689983,
      "grad_norm": 6.931854248046875,
      "learning_rate": 4.7036158861885006e-05,
      "loss": 0.5793,
      "step": 300
    },
    {
      "epoch": 0.18375815056312983,
      "grad_norm": 20.177288055419922,
      "learning_rate": 4.6937364157281174e-05,
      "loss": 0.4974,
      "step": 310
    },
    {
      "epoch": 0.1896858328393598,
      "grad_norm": 9.56718635559082,
      "learning_rate": 4.6838569452677335e-05,
      "loss": 0.5469,
      "step": 320
    },
    {
      "epoch": 0.1956135151155898,
      "grad_norm": 68.67320251464844,
      "learning_rate": 4.6739774748073503e-05,
      "loss": 0.5815,
      "step": 330
    },
    {
      "epoch": 0.2015411973918198,
      "grad_norm": 122.79833221435547,
      "learning_rate": 4.664098004346967e-05,
      "loss": 0.5245,
      "step": 340
    },
    {
      "epoch": 0.2074688796680498,
      "grad_norm": 78.1559829711914,
      "learning_rate": 4.654218533886584e-05,
      "loss": 0.6321,
      "step": 350
    },
    {
      "epoch": 0.21339656194427978,
      "grad_norm": 24.242849349975586,
      "learning_rate": 4.644339063426201e-05,
      "loss": 0.5814,
      "step": 360
    },
    {
      "epoch": 0.21932424422050978,
      "grad_norm": 36.89963912963867,
      "learning_rate": 4.6344595929658175e-05,
      "loss": 0.651,
      "step": 370
    },
    {
      "epoch": 0.22525192649673978,
      "grad_norm": 20.859704971313477,
      "learning_rate": 4.624580122505434e-05,
      "loss": 0.6346,
      "step": 380
    },
    {
      "epoch": 0.23117960877296978,
      "grad_norm": 24.917713165283203,
      "learning_rate": 4.614700652045051e-05,
      "loss": 0.6302,
      "step": 390
    },
    {
      "epoch": 0.23710729104919975,
      "grad_norm": 29.016895294189453,
      "learning_rate": 4.604821181584667e-05,
      "loss": 0.5671,
      "step": 400
    },
    {
      "epoch": 0.24303497332542975,
      "grad_norm": 12.832783699035645,
      "learning_rate": 4.594941711124284e-05,
      "loss": 0.5664,
      "step": 410
    },
    {
      "epoch": 0.24896265560165975,
      "grad_norm": 11.57109546661377,
      "learning_rate": 4.5850622406639e-05,
      "loss": 0.5259,
      "step": 420
    },
    {
      "epoch": 0.2548903378778897,
      "grad_norm": 14.475279808044434,
      "learning_rate": 4.575182770203517e-05,
      "loss": 0.5819,
      "step": 430
    },
    {
      "epoch": 0.26081802015411976,
      "grad_norm": 63.90249252319336,
      "learning_rate": 4.565303299743134e-05,
      "loss": 0.6211,
      "step": 440
    },
    {
      "epoch": 0.26674570243034973,
      "grad_norm": 73.16571044921875,
      "learning_rate": 4.5554238292827505e-05,
      "loss": 0.5551,
      "step": 450
    },
    {
      "epoch": 0.2726733847065797,
      "grad_norm": 36.59919357299805,
      "learning_rate": 4.545544358822367e-05,
      "loss": 0.5478,
      "step": 460
    },
    {
      "epoch": 0.27860106698280973,
      "grad_norm": 27.131576538085938,
      "learning_rate": 4.535664888361984e-05,
      "loss": 0.6151,
      "step": 470
    },
    {
      "epoch": 0.2845287492590397,
      "grad_norm": 14.471365928649902,
      "learning_rate": 4.525785417901601e-05,
      "loss": 0.5712,
      "step": 480
    },
    {
      "epoch": 0.29045643153526973,
      "grad_norm": 13.836601257324219,
      "learning_rate": 4.515905947441218e-05,
      "loss": 0.5986,
      "step": 490
    },
    {
      "epoch": 0.2963841138114997,
      "grad_norm": 9.851883888244629,
      "learning_rate": 4.5060264769808345e-05,
      "loss": 0.6126,
      "step": 500
    },
    {
      "epoch": 0.3023117960877297,
      "grad_norm": 8.354625701904297,
      "learning_rate": 4.4961470065204506e-05,
      "loss": 0.521,
      "step": 510
    },
    {
      "epoch": 0.3082394783639597,
      "grad_norm": 19.55716323852539,
      "learning_rate": 4.4862675360600674e-05,
      "loss": 0.5017,
      "step": 520
    },
    {
      "epoch": 0.3141671606401897,
      "grad_norm": 17.373065948486328,
      "learning_rate": 4.476388065599684e-05,
      "loss": 0.4918,
      "step": 530
    },
    {
      "epoch": 0.32009484291641965,
      "grad_norm": 12.192534446716309,
      "learning_rate": 4.466508595139301e-05,
      "loss": 0.5234,
      "step": 540
    },
    {
      "epoch": 0.3260225251926497,
      "grad_norm": 21.954313278198242,
      "learning_rate": 4.456629124678918e-05,
      "loss": 0.5151,
      "step": 550
    },
    {
      "epoch": 0.33195020746887965,
      "grad_norm": 16.208221435546875,
      "learning_rate": 4.446749654218534e-05,
      "loss": 0.5937,
      "step": 560
    },
    {
      "epoch": 0.3378778897451097,
      "grad_norm": 55.068504333496094,
      "learning_rate": 4.436870183758151e-05,
      "loss": 0.5697,
      "step": 570
    },
    {
      "epoch": 0.34380557202133966,
      "grad_norm": 45.51179122924805,
      "learning_rate": 4.4269907132977675e-05,
      "loss": 0.5129,
      "step": 580
    },
    {
      "epoch": 0.34973325429756963,
      "grad_norm": 12.437984466552734,
      "learning_rate": 4.417111242837384e-05,
      "loss": 0.6185,
      "step": 590
    },
    {
      "epoch": 0.35566093657379966,
      "grad_norm": 18.581085205078125,
      "learning_rate": 4.4072317723770004e-05,
      "loss": 0.5165,
      "step": 600
    },
    {
      "epoch": 0.36158861885002963,
      "grad_norm": 23.36187744140625,
      "learning_rate": 4.397352301916617e-05,
      "loss": 0.5115,
      "step": 610
    },
    {
      "epoch": 0.36751630112625966,
      "grad_norm": 13.504555702209473,
      "learning_rate": 4.387472831456234e-05,
      "loss": 0.5639,
      "step": 620
    },
    {
      "epoch": 0.37344398340248963,
      "grad_norm": 31.79749870300293,
      "learning_rate": 4.377593360995851e-05,
      "loss": 0.5043,
      "step": 630
    },
    {
      "epoch": 0.3793716656787196,
      "grad_norm": 38.95268249511719,
      "learning_rate": 4.3677138905354675e-05,
      "loss": 0.5202,
      "step": 640
    },
    {
      "epoch": 0.38529934795494963,
      "grad_norm": 24.532787322998047,
      "learning_rate": 4.357834420075084e-05,
      "loss": 0.4589,
      "step": 650
    },
    {
      "epoch": 0.3912270302311796,
      "grad_norm": 12.012605667114258,
      "learning_rate": 4.347954949614701e-05,
      "loss": 0.5539,
      "step": 660
    },
    {
      "epoch": 0.3971547125074096,
      "grad_norm": 11.958657264709473,
      "learning_rate": 4.338075479154318e-05,
      "loss": 0.4616,
      "step": 670
    },
    {
      "epoch": 0.4030823947836396,
      "grad_norm": 40.62808609008789,
      "learning_rate": 4.328196008693935e-05,
      "loss": 0.5088,
      "step": 680
    },
    {
      "epoch": 0.4090100770598696,
      "grad_norm": 47.91569900512695,
      "learning_rate": 4.318316538233551e-05,
      "loss": 0.5183,
      "step": 690
    },
    {
      "epoch": 0.4149377593360996,
      "grad_norm": 16.3569278717041,
      "learning_rate": 4.3084370677731676e-05,
      "loss": 0.5828,
      "step": 700
    },
    {
      "epoch": 0.4208654416123296,
      "grad_norm": 47.268375396728516,
      "learning_rate": 4.2985575973127844e-05,
      "loss": 0.4905,
      "step": 710
    },
    {
      "epoch": 0.42679312388855956,
      "grad_norm": 71.96983337402344,
      "learning_rate": 4.2886781268524005e-05,
      "loss": 0.5331,
      "step": 720
    },
    {
      "epoch": 0.4327208061647896,
      "grad_norm": 5.641787528991699,
      "learning_rate": 4.278798656392017e-05,
      "loss": 0.6428,
      "step": 730
    },
    {
      "epoch": 0.43864848844101956,
      "grad_norm": 51.863365173339844,
      "learning_rate": 4.268919185931634e-05,
      "loss": 0.4849,
      "step": 740
    },
    {
      "epoch": 0.44457617071724953,
      "grad_norm": 37.18937301635742,
      "learning_rate": 4.259039715471251e-05,
      "loss": 0.5447,
      "step": 750
    },
    {
      "epoch": 0.45050385299347956,
      "grad_norm": 11.722724914550781,
      "learning_rate": 4.249160245010868e-05,
      "loss": 0.5476,
      "step": 760
    },
    {
      "epoch": 0.45643153526970953,
      "grad_norm": 43.98824691772461,
      "learning_rate": 4.2392807745504845e-05,
      "loss": 0.6262,
      "step": 770
    },
    {
      "epoch": 0.46235921754593956,
      "grad_norm": 8.402789115905762,
      "learning_rate": 4.2294013040901006e-05,
      "loss": 0.5162,
      "step": 780
    },
    {
      "epoch": 0.46828689982216953,
      "grad_norm": 33.1689567565918,
      "learning_rate": 4.2195218336297174e-05,
      "loss": 0.5358,
      "step": 790
    },
    {
      "epoch": 0.4742145820983995,
      "grad_norm": 15.662578582763672,
      "learning_rate": 4.209642363169334e-05,
      "loss": 0.4971,
      "step": 800
    },
    {
      "epoch": 0.48014226437462953,
      "grad_norm": 32.71614074707031,
      "learning_rate": 4.199762892708951e-05,
      "loss": 0.4791,
      "step": 810
    },
    {
      "epoch": 0.4860699466508595,
      "grad_norm": 13.527876853942871,
      "learning_rate": 4.189883422248568e-05,
      "loss": 0.4655,
      "step": 820
    },
    {
      "epoch": 0.4919976289270895,
      "grad_norm": 8.002348899841309,
      "learning_rate": 4.1800039517881846e-05,
      "loss": 0.5425,
      "step": 830
    },
    {
      "epoch": 0.4979253112033195,
      "grad_norm": 42.055450439453125,
      "learning_rate": 4.1701244813278014e-05,
      "loss": 0.5944,
      "step": 840
    },
    {
      "epoch": 0.5038529934795495,
      "grad_norm": 6.069934368133545,
      "learning_rate": 4.160245010867418e-05,
      "loss": 0.6239,
      "step": 850
    },
    {
      "epoch": 0.5097806757557795,
      "grad_norm": 6.783214569091797,
      "learning_rate": 4.150365540407035e-05,
      "loss": 0.65,
      "step": 860
    },
    {
      "epoch": 0.5157083580320095,
      "grad_norm": 75.57958221435547,
      "learning_rate": 4.140486069946651e-05,
      "loss": 0.4999,
      "step": 870
    },
    {
      "epoch": 0.5216360403082395,
      "grad_norm": 14.70267105102539,
      "learning_rate": 4.130606599486267e-05,
      "loss": 0.5237,
      "step": 880
    },
    {
      "epoch": 0.5275637225844695,
      "grad_norm": 49.0606803894043,
      "learning_rate": 4.120727129025884e-05,
      "loss": 0.5604,
      "step": 890
    },
    {
      "epoch": 0.5334914048606995,
      "grad_norm": 5.73143196105957,
      "learning_rate": 4.110847658565501e-05,
      "loss": 0.5571,
      "step": 900
    },
    {
      "epoch": 0.5394190871369294,
      "grad_norm": 8.64951229095459,
      "learning_rate": 4.1009681881051176e-05,
      "loss": 0.5124,
      "step": 910
    },
    {
      "epoch": 0.5453467694131594,
      "grad_norm": 25.14156150817871,
      "learning_rate": 4.0910887176447343e-05,
      "loss": 0.6028,
      "step": 920
    },
    {
      "epoch": 0.5512744516893895,
      "grad_norm": 6.401345729827881,
      "learning_rate": 4.081209247184351e-05,
      "loss": 0.5774,
      "step": 930
    },
    {
      "epoch": 0.5572021339656195,
      "grad_norm": 8.712220191955566,
      "learning_rate": 4.071329776723968e-05,
      "loss": 0.5753,
      "step": 940
    },
    {
      "epoch": 0.5631298162418494,
      "grad_norm": 11.452255249023438,
      "learning_rate": 4.061450306263585e-05,
      "loss": 0.5115,
      "step": 950
    },
    {
      "epoch": 0.5690574985180794,
      "grad_norm": 10.3247652053833,
      "learning_rate": 4.051570835803201e-05,
      "loss": 0.457,
      "step": 960
    },
    {
      "epoch": 0.5749851807943094,
      "grad_norm": 8.051189422607422,
      "learning_rate": 4.0416913653428176e-05,
      "loss": 0.6535,
      "step": 970
    },
    {
      "epoch": 0.5809128630705395,
      "grad_norm": 7.823803901672363,
      "learning_rate": 4.0318118948824344e-05,
      "loss": 0.5654,
      "step": 980
    },
    {
      "epoch": 0.5868405453467694,
      "grad_norm": 2.754333019256592,
      "learning_rate": 4.021932424422051e-05,
      "loss": 0.561,
      "step": 990
    },
    {
      "epoch": 0.5927682276229994,
      "grad_norm": 3.2010560035705566,
      "learning_rate": 4.012052953961668e-05,
      "loss": 0.6191,
      "step": 1000
    },
    {
      "epoch": 0.5986959098992294,
      "grad_norm": 20.963420867919922,
      "learning_rate": 4.002173483501285e-05,
      "loss": 0.5632,
      "step": 1010
    },
    {
      "epoch": 0.6046235921754594,
      "grad_norm": 7.515842437744141,
      "learning_rate": 3.9922940130409016e-05,
      "loss": 0.561,
      "step": 1020
    },
    {
      "epoch": 0.6105512744516894,
      "grad_norm": 58.44517517089844,
      "learning_rate": 3.982414542580518e-05,
      "loss": 0.5497,
      "step": 1030
    },
    {
      "epoch": 0.6164789567279194,
      "grad_norm": 18.94573211669922,
      "learning_rate": 3.9725350721201345e-05,
      "loss": 0.4113,
      "step": 1040
    },
    {
      "epoch": 0.6224066390041494,
      "grad_norm": 238.8856964111328,
      "learning_rate": 3.962655601659751e-05,
      "loss": 0.518,
      "step": 1050
    },
    {
      "epoch": 0.6283343212803794,
      "grad_norm": 101.71446990966797,
      "learning_rate": 3.9527761311993674e-05,
      "loss": 0.5327,
      "step": 1060
    },
    {
      "epoch": 0.6342620035566093,
      "grad_norm": 6.219387054443359,
      "learning_rate": 3.942896660738984e-05,
      "loss": 0.5326,
      "step": 1070
    },
    {
      "epoch": 0.6401896858328393,
      "grad_norm": 5.771879196166992,
      "learning_rate": 3.933017190278601e-05,
      "loss": 0.4716,
      "step": 1080
    },
    {
      "epoch": 0.6461173681090694,
      "grad_norm": 32.685272216796875,
      "learning_rate": 3.923137719818218e-05,
      "loss": 0.5534,
      "step": 1090
    },
    {
      "epoch": 0.6520450503852994,
      "grad_norm": 19.75934410095215,
      "learning_rate": 3.9132582493578346e-05,
      "loss": 0.4991,
      "step": 1100
    },
    {
      "epoch": 0.6579727326615293,
      "grad_norm": 17.99289894104004,
      "learning_rate": 3.9033787788974514e-05,
      "loss": 0.576,
      "step": 1110
    },
    {
      "epoch": 0.6639004149377593,
      "grad_norm": 38.65797805786133,
      "learning_rate": 3.893499308437068e-05,
      "loss": 0.4983,
      "step": 1120
    },
    {
      "epoch": 0.6698280972139893,
      "grad_norm": 6.34208345413208,
      "learning_rate": 3.883619837976685e-05,
      "loss": 0.4788,
      "step": 1130
    },
    {
      "epoch": 0.6757557794902194,
      "grad_norm": 16.977649688720703,
      "learning_rate": 3.873740367516302e-05,
      "loss": 0.4308,
      "step": 1140
    },
    {
      "epoch": 0.6816834617664493,
      "grad_norm": 10.427937507629395,
      "learning_rate": 3.863860897055918e-05,
      "loss": 0.5163,
      "step": 1150
    },
    {
      "epoch": 0.6876111440426793,
      "grad_norm": 228.77536010742188,
      "learning_rate": 3.853981426595535e-05,
      "loss": 0.5197,
      "step": 1160
    },
    {
      "epoch": 0.6935388263189093,
      "grad_norm": 12.161484718322754,
      "learning_rate": 3.8441019561351515e-05,
      "loss": 0.5464,
      "step": 1170
    },
    {
      "epoch": 0.6994665085951393,
      "grad_norm": 6.431323528289795,
      "learning_rate": 3.834222485674768e-05,
      "loss": 0.457,
      "step": 1180
    },
    {
      "epoch": 0.7053941908713693,
      "grad_norm": 4.493706226348877,
      "learning_rate": 3.8243430152143844e-05,
      "loss": 0.6127,
      "step": 1190
    },
    {
      "epoch": 0.7113218731475993,
      "grad_norm": 104.3836441040039,
      "learning_rate": 3.814463544754001e-05,
      "loss": 0.5101,
      "step": 1200
    },
    {
      "epoch": 0.7172495554238293,
      "grad_norm": 5.14698600769043,
      "learning_rate": 3.804584074293618e-05,
      "loss": 0.4577,
      "step": 1210
    },
    {
      "epoch": 0.7231772377000593,
      "grad_norm": 23.63407325744629,
      "learning_rate": 3.794704603833235e-05,
      "loss": 0.4613,
      "step": 1220
    },
    {
      "epoch": 0.7291049199762892,
      "grad_norm": 11.728093147277832,
      "learning_rate": 3.7848251333728515e-05,
      "loss": 0.4911,
      "step": 1230
    },
    {
      "epoch": 0.7350326022525193,
      "grad_norm": 45.91849899291992,
      "learning_rate": 3.7749456629124677e-05,
      "loss": 0.485,
      "step": 1240
    },
    {
      "epoch": 0.7409602845287493,
      "grad_norm": 12.272936820983887,
      "learning_rate": 3.7650661924520845e-05,
      "loss": 0.468,
      "step": 1250
    },
    {
      "epoch": 0.7468879668049793,
      "grad_norm": 18.5985050201416,
      "learning_rate": 3.755186721991701e-05,
      "loss": 0.5322,
      "step": 1260
    },
    {
      "epoch": 0.7528156490812092,
      "grad_norm": 29.861644744873047,
      "learning_rate": 3.745307251531318e-05,
      "loss": 0.5181,
      "step": 1270
    },
    {
      "epoch": 0.7587433313574392,
      "grad_norm": 13.890777587890625,
      "learning_rate": 3.735427781070935e-05,
      "loss": 0.4041,
      "step": 1280
    },
    {
      "epoch": 0.7646710136336692,
      "grad_norm": 48.92064666748047,
      "learning_rate": 3.7255483106105516e-05,
      "loss": 0.5757,
      "step": 1290
    },
    {
      "epoch": 0.7705986959098993,
      "grad_norm": 11.369391441345215,
      "learning_rate": 3.7156688401501684e-05,
      "loss": 0.5061,
      "step": 1300
    },
    {
      "epoch": 0.7765263781861292,
      "grad_norm": 4.402633190155029,
      "learning_rate": 3.705789369689785e-05,
      "loss": 0.5023,
      "step": 1310
    },
    {
      "epoch": 0.7824540604623592,
      "grad_norm": 31.41122055053711,
      "learning_rate": 3.695909899229402e-05,
      "loss": 0.5182,
      "step": 1320
    },
    {
      "epoch": 0.7883817427385892,
      "grad_norm": 16.837535858154297,
      "learning_rate": 3.686030428769018e-05,
      "loss": 0.6012,
      "step": 1330
    },
    {
      "epoch": 0.7943094250148192,
      "grad_norm": 17.68408966064453,
      "learning_rate": 3.676150958308635e-05,
      "loss": 0.5181,
      "step": 1340
    },
    {
      "epoch": 0.8002371072910492,
      "grad_norm": 28.281538009643555,
      "learning_rate": 3.666271487848251e-05,
      "loss": 0.5188,
      "step": 1350
    },
    {
      "epoch": 0.8061647895672792,
      "grad_norm": 9.940011024475098,
      "learning_rate": 3.656392017387868e-05,
      "loss": 0.4345,
      "step": 1360
    },
    {
      "epoch": 0.8120924718435092,
      "grad_norm": 293.8431701660156,
      "learning_rate": 3.6465125469274846e-05,
      "loss": 0.4613,
      "step": 1370
    },
    {
      "epoch": 0.8180201541197392,
      "grad_norm": 20.543962478637695,
      "learning_rate": 3.6366330764671014e-05,
      "loss": 0.4853,
      "step": 1380
    },
    {
      "epoch": 0.8239478363959691,
      "grad_norm": 29.52580451965332,
      "learning_rate": 3.626753606006718e-05,
      "loss": 0.5348,
      "step": 1390
    },
    {
      "epoch": 0.8298755186721992,
      "grad_norm": 87.13895416259766,
      "learning_rate": 3.616874135546335e-05,
      "loss": 0.4216,
      "step": 1400
    },
    {
      "epoch": 0.8358032009484292,
      "grad_norm": 7.074516773223877,
      "learning_rate": 3.606994665085952e-05,
      "loss": 0.5355,
      "step": 1410
    },
    {
      "epoch": 0.8417308832246592,
      "grad_norm": 323.1661376953125,
      "learning_rate": 3.597115194625568e-05,
      "loss": 0.4674,
      "step": 1420
    },
    {
      "epoch": 0.8476585655008891,
      "grad_norm": 11.765792846679688,
      "learning_rate": 3.587235724165185e-05,
      "loss": 0.5105,
      "step": 1430
    },
    {
      "epoch": 0.8535862477771191,
      "grad_norm": 13.943761825561523,
      "learning_rate": 3.5773562537048015e-05,
      "loss": 0.506,
      "step": 1440
    },
    {
      "epoch": 0.8595139300533492,
      "grad_norm": 7.428855895996094,
      "learning_rate": 3.567476783244418e-05,
      "loss": 0.5411,
      "step": 1450
    },
    {
      "epoch": 0.8654416123295792,
      "grad_norm": 7.060905456542969,
      "learning_rate": 3.557597312784035e-05,
      "loss": 0.4916,
      "step": 1460
    },
    {
      "epoch": 0.8713692946058091,
      "grad_norm": 6.235435485839844,
      "learning_rate": 3.547717842323652e-05,
      "loss": 0.3408,
      "step": 1470
    },
    {
      "epoch": 0.8772969768820391,
      "grad_norm": 12.985630989074707,
      "learning_rate": 3.5378383718632687e-05,
      "loss": 0.5413,
      "step": 1480
    },
    {
      "epoch": 0.8832246591582691,
      "grad_norm": 16.37630844116211,
      "learning_rate": 3.5279589014028855e-05,
      "loss": 0.5489,
      "step": 1490
    },
    {
      "epoch": 0.8891523414344991,
      "grad_norm": 6.321455001831055,
      "learning_rate": 3.518079430942502e-05,
      "loss": 0.4749,
      "step": 1500
    },
    {
      "epoch": 0.8950800237107291,
      "grad_norm": 164.0301055908203,
      "learning_rate": 3.5081999604821184e-05,
      "loss": 0.4344,
      "step": 1510
    },
    {
      "epoch": 0.9010077059869591,
      "grad_norm": 5.860137939453125,
      "learning_rate": 3.4983204900217345e-05,
      "loss": 0.4825,
      "step": 1520
    },
    {
      "epoch": 0.9069353882631891,
      "grad_norm": 6.089977741241455,
      "learning_rate": 3.488441019561351e-05,
      "loss": 0.4533,
      "step": 1530
    },
    {
      "epoch": 0.9128630705394191,
      "grad_norm": 24.377971649169922,
      "learning_rate": 3.478561549100968e-05,
      "loss": 0.4752,
      "step": 1540
    },
    {
      "epoch": 0.918790752815649,
      "grad_norm": 20.785724639892578,
      "learning_rate": 3.468682078640585e-05,
      "loss": 0.5295,
      "step": 1550
    },
    {
      "epoch": 0.9247184350918791,
      "grad_norm": 57.24374771118164,
      "learning_rate": 3.4588026081802016e-05,
      "loss": 0.7653,
      "step": 1560
    },
    {
      "epoch": 0.9306461173681091,
      "grad_norm": 5.909299850463867,
      "learning_rate": 3.4489231377198184e-05,
      "loss": 0.5072,
      "step": 1570
    },
    {
      "epoch": 0.9365737996443391,
      "grad_norm": 745.7210083007812,
      "learning_rate": 3.439043667259435e-05,
      "loss": 0.4866,
      "step": 1580
    },
    {
      "epoch": 0.942501481920569,
      "grad_norm": 7.1468329429626465,
      "learning_rate": 3.429164196799052e-05,
      "loss": 0.4974,
      "step": 1590
    },
    {
      "epoch": 0.948429164196799,
      "grad_norm": 8.773387908935547,
      "learning_rate": 3.419284726338668e-05,
      "loss": 0.5335,
      "step": 1600
    },
    {
      "epoch": 0.9543568464730291,
      "grad_norm": 13.26337718963623,
      "learning_rate": 3.409405255878285e-05,
      "loss": 0.4622,
      "step": 1610
    },
    {
      "epoch": 0.9602845287492591,
      "grad_norm": 8.9894380569458,
      "learning_rate": 3.399525785417902e-05,
      "loss": 0.5418,
      "step": 1620
    },
    {
      "epoch": 0.966212211025489,
      "grad_norm": 21.272056579589844,
      "learning_rate": 3.3896463149575185e-05,
      "loss": 0.5654,
      "step": 1630
    },
    {
      "epoch": 0.972139893301719,
      "grad_norm": 6.448025226593018,
      "learning_rate": 3.379766844497135e-05,
      "loss": 0.5242,
      "step": 1640
    },
    {
      "epoch": 0.978067575577949,
      "grad_norm": 6.166467189788818,
      "learning_rate": 3.369887374036752e-05,
      "loss": 0.5178,
      "step": 1650
    },
    {
      "epoch": 0.983995257854179,
      "grad_norm": 290.25091552734375,
      "learning_rate": 3.360007903576369e-05,
      "loss": 0.4835,
      "step": 1660
    },
    {
      "epoch": 0.989922940130409,
      "grad_norm": 9.350057601928711,
      "learning_rate": 3.350128433115985e-05,
      "loss": 0.537,
      "step": 1670
    },
    {
      "epoch": 0.995850622406639,
      "grad_norm": 6.101749897003174,
      "learning_rate": 3.340248962655602e-05,
      "loss": 0.4872,
      "step": 1680
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.7745330566261488,
      "eval_f1": 0.7786992579659537,
      "eval_loss": 0.4941551387310028,
      "eval_precision": 0.6907589055240062,
      "eval_recall": 0.8922974324774925,
      "eval_runtime": 13.676,
      "eval_samples_per_second": 493.272,
      "eval_steps_per_second": 30.857,
      "step": 1687
    },
    {
      "epoch": 1.001778304682869,
      "grad_norm": 3.759908437728882,
      "learning_rate": 3.3303694921952186e-05,
      "loss": 0.5389,
      "step": 1690
    },
    {
      "epoch": 1.007705986959099,
      "grad_norm": 8.236708641052246,
      "learning_rate": 3.320490021734835e-05,
      "loss": 0.4566,
      "step": 1700
    },
    {
      "epoch": 1.013633669235329,
      "grad_norm": 69.06660461425781,
      "learning_rate": 3.3106105512744515e-05,
      "loss": 0.5285,
      "step": 1710
    },
    {
      "epoch": 1.019561351511559,
      "grad_norm": 16.272361755371094,
      "learning_rate": 3.300731080814068e-05,
      "loss": 0.4837,
      "step": 1720
    },
    {
      "epoch": 1.0254890337877889,
      "grad_norm": 14.366669654846191,
      "learning_rate": 3.290851610353685e-05,
      "loss": 0.3924,
      "step": 1730
    },
    {
      "epoch": 1.0314167160640189,
      "grad_norm": 20.169204711914062,
      "learning_rate": 3.280972139893302e-05,
      "loss": 0.4331,
      "step": 1740
    },
    {
      "epoch": 1.037344398340249,
      "grad_norm": 11.239407539367676,
      "learning_rate": 3.271092669432919e-05,
      "loss": 0.4211,
      "step": 1750
    },
    {
      "epoch": 1.043272080616479,
      "grad_norm": 36.23087692260742,
      "learning_rate": 3.2612131989725355e-05,
      "loss": 0.4254,
      "step": 1760
    },
    {
      "epoch": 1.049199762892709,
      "grad_norm": 49.828895568847656,
      "learning_rate": 3.251333728512152e-05,
      "loss": 0.3081,
      "step": 1770
    },
    {
      "epoch": 1.055127445168939,
      "grad_norm": 59.408931732177734,
      "learning_rate": 3.241454258051769e-05,
      "loss": 0.4051,
      "step": 1780
    },
    {
      "epoch": 1.061055127445169,
      "grad_norm": 30.569923400878906,
      "learning_rate": 3.231574787591385e-05,
      "loss": 0.5528,
      "step": 1790
    },
    {
      "epoch": 1.066982809721399,
      "grad_norm": 10.299112319946289,
      "learning_rate": 3.221695317131002e-05,
      "loss": 0.4943,
      "step": 1800
    },
    {
      "epoch": 1.072910491997629,
      "grad_norm": 9.296185493469238,
      "learning_rate": 3.211815846670619e-05,
      "loss": 0.4128,
      "step": 1810
    },
    {
      "epoch": 1.0788381742738589,
      "grad_norm": 14.046798706054688,
      "learning_rate": 3.201936376210235e-05,
      "loss": 0.4914,
      "step": 1820
    },
    {
      "epoch": 1.0847658565500888,
      "grad_norm": 28.382823944091797,
      "learning_rate": 3.192056905749852e-05,
      "loss": 0.5306,
      "step": 1830
    },
    {
      "epoch": 1.0906935388263188,
      "grad_norm": 9.328556060791016,
      "learning_rate": 3.1821774352894685e-05,
      "loss": 0.4574,
      "step": 1840
    },
    {
      "epoch": 1.096621221102549,
      "grad_norm": 28.29318618774414,
      "learning_rate": 3.172297964829085e-05,
      "loss": 0.4663,
      "step": 1850
    },
    {
      "epoch": 1.102548903378779,
      "grad_norm": 23.488359451293945,
      "learning_rate": 3.162418494368702e-05,
      "loss": 0.4443,
      "step": 1860
    },
    {
      "epoch": 1.108476585655009,
      "grad_norm": 8.609654426574707,
      "learning_rate": 3.152539023908319e-05,
      "loss": 0.4893,
      "step": 1870
    },
    {
      "epoch": 1.114404267931239,
      "grad_norm": 27.786441802978516,
      "learning_rate": 3.142659553447935e-05,
      "loss": 0.5285,
      "step": 1880
    },
    {
      "epoch": 1.120331950207469,
      "grad_norm": 9.73981761932373,
      "learning_rate": 3.132780082987552e-05,
      "loss": 0.5305,
      "step": 1890
    },
    {
      "epoch": 1.1262596324836989,
      "grad_norm": 8.289258003234863,
      "learning_rate": 3.1229006125271685e-05,
      "loss": 0.4891,
      "step": 1900
    },
    {
      "epoch": 1.1321873147599288,
      "grad_norm": 77.18204498291016,
      "learning_rate": 3.113021142066785e-05,
      "loss": 0.5287,
      "step": 1910
    },
    {
      "epoch": 1.1381149970361588,
      "grad_norm": 12.229536056518555,
      "learning_rate": 3.103141671606402e-05,
      "loss": 0.502,
      "step": 1920
    },
    {
      "epoch": 1.1440426793123888,
      "grad_norm": 13.459188461303711,
      "learning_rate": 3.093262201146019e-05,
      "loss": 0.5106,
      "step": 1930
    },
    {
      "epoch": 1.1499703615886188,
      "grad_norm": 16.00262451171875,
      "learning_rate": 3.083382730685636e-05,
      "loss": 0.4565,
      "step": 1940
    },
    {
      "epoch": 1.1558980438648487,
      "grad_norm": 8.601886749267578,
      "learning_rate": 3.0735032602252525e-05,
      "loss": 0.4935,
      "step": 1950
    },
    {
      "epoch": 1.161825726141079,
      "grad_norm": 17.20250701904297,
      "learning_rate": 3.063623789764869e-05,
      "loss": 0.4026,
      "step": 1960
    },
    {
      "epoch": 1.167753408417309,
      "grad_norm": 9.497243881225586,
      "learning_rate": 3.0537443193044854e-05,
      "loss": 0.4589,
      "step": 1970
    },
    {
      "epoch": 1.1736810906935389,
      "grad_norm": 6.127623558044434,
      "learning_rate": 3.043864848844102e-05,
      "loss": 0.3799,
      "step": 1980
    },
    {
      "epoch": 1.1796087729697688,
      "grad_norm": 26.308931350708008,
      "learning_rate": 3.0339853783837187e-05,
      "loss": 0.44,
      "step": 1990
    },
    {
      "epoch": 1.1855364552459988,
      "grad_norm": 29.8370418548584,
      "learning_rate": 3.024105907923335e-05,
      "loss": 0.4213,
      "step": 2000
    },
    {
      "epoch": 1.1914641375222288,
      "grad_norm": 17.87353515625,
      "learning_rate": 3.014226437462952e-05,
      "loss": 0.4491,
      "step": 2010
    },
    {
      "epoch": 1.1973918197984588,
      "grad_norm": 14.055922508239746,
      "learning_rate": 3.0043469670025687e-05,
      "loss": 0.506,
      "step": 2020
    },
    {
      "epoch": 1.2033195020746887,
      "grad_norm": 7.607759475708008,
      "learning_rate": 2.9944674965421855e-05,
      "loss": 0.3684,
      "step": 2030
    },
    {
      "epoch": 1.2092471843509187,
      "grad_norm": 26.028057098388672,
      "learning_rate": 2.984588026081802e-05,
      "loss": 0.5432,
      "step": 2040
    },
    {
      "epoch": 1.215174866627149,
      "grad_norm": 157.5063018798828,
      "learning_rate": 2.9747085556214187e-05,
      "loss": 0.5399,
      "step": 2050
    },
    {
      "epoch": 1.2211025489033789,
      "grad_norm": 17.382211685180664,
      "learning_rate": 2.9648290851610355e-05,
      "loss": 0.4722,
      "step": 2060
    },
    {
      "epoch": 1.2270302311796089,
      "grad_norm": 52.982967376708984,
      "learning_rate": 2.9549496147006523e-05,
      "loss": 0.457,
      "step": 2070
    },
    {
      "epoch": 1.2329579134558388,
      "grad_norm": 20.936004638671875,
      "learning_rate": 2.9450701442402688e-05,
      "loss": 0.5055,
      "step": 2080
    },
    {
      "epoch": 1.2388855957320688,
      "grad_norm": 13.01406478881836,
      "learning_rate": 2.9351906737798856e-05,
      "loss": 0.4871,
      "step": 2090
    },
    {
      "epoch": 1.2448132780082988,
      "grad_norm": 3.669673204421997,
      "learning_rate": 2.9253112033195024e-05,
      "loss": 0.4411,
      "step": 2100
    },
    {
      "epoch": 1.2507409602845287,
      "grad_norm": 20.690616607666016,
      "learning_rate": 2.915431732859119e-05,
      "loss": 0.3809,
      "step": 2110
    },
    {
      "epoch": 1.2566686425607587,
      "grad_norm": 6.880071640014648,
      "learning_rate": 2.905552262398736e-05,
      "loss": 0.5685,
      "step": 2120
    },
    {
      "epoch": 1.2625963248369887,
      "grad_norm": 33.478763580322266,
      "learning_rate": 2.8956727919383524e-05,
      "loss": 0.4663,
      "step": 2130
    },
    {
      "epoch": 1.2685240071132187,
      "grad_norm": 7.821342468261719,
      "learning_rate": 2.8857933214779685e-05,
      "loss": 0.4236,
      "step": 2140
    },
    {
      "epoch": 1.2744516893894486,
      "grad_norm": 11.465217590332031,
      "learning_rate": 2.8759138510175853e-05,
      "loss": 0.6586,
      "step": 2150
    },
    {
      "epoch": 1.2803793716656786,
      "grad_norm": 19.353574752807617,
      "learning_rate": 2.866034380557202e-05,
      "loss": 0.4921,
      "step": 2160
    },
    {
      "epoch": 1.2863070539419086,
      "grad_norm": 16.753293991088867,
      "learning_rate": 2.856154910096819e-05,
      "loss": 0.4853,
      "step": 2170
    },
    {
      "epoch": 1.2922347362181388,
      "grad_norm": 16.991769790649414,
      "learning_rate": 2.8462754396364354e-05,
      "loss": 0.4044,
      "step": 2180
    },
    {
      "epoch": 1.2981624184943688,
      "grad_norm": 4.9346604347229,
      "learning_rate": 2.836395969176052e-05,
      "loss": 0.4848,
      "step": 2190
    },
    {
      "epoch": 1.3040901007705987,
      "grad_norm": 14.47732925415039,
      "learning_rate": 2.826516498715669e-05,
      "loss": 0.3496,
      "step": 2200
    },
    {
      "epoch": 1.3100177830468287,
      "grad_norm": 11.453994750976562,
      "learning_rate": 2.8166370282552857e-05,
      "loss": 0.3892,
      "step": 2210
    },
    {
      "epoch": 1.3159454653230587,
      "grad_norm": 14.126304626464844,
      "learning_rate": 2.8067575577949022e-05,
      "loss": 0.5153,
      "step": 2220
    },
    {
      "epoch": 1.3218731475992886,
      "grad_norm": 10.463335037231445,
      "learning_rate": 2.796878087334519e-05,
      "loss": 0.3977,
      "step": 2230
    },
    {
      "epoch": 1.3278008298755186,
      "grad_norm": 11.16727066040039,
      "learning_rate": 2.7869986168741358e-05,
      "loss": 0.439,
      "step": 2240
    },
    {
      "epoch": 1.3337285121517486,
      "grad_norm": 25.198686599731445,
      "learning_rate": 2.7771191464137526e-05,
      "loss": 0.384,
      "step": 2250
    },
    {
      "epoch": 1.3396561944279788,
      "grad_norm": 10.64380168914795,
      "learning_rate": 2.7672396759533694e-05,
      "loss": 0.5976,
      "step": 2260
    },
    {
      "epoch": 1.3455838767042088,
      "grad_norm": 7.170768737792969,
      "learning_rate": 2.7573602054929858e-05,
      "loss": 0.3326,
      "step": 2270
    },
    {
      "epoch": 1.3515115589804387,
      "grad_norm": 9.482901573181152,
      "learning_rate": 2.7474807350326026e-05,
      "loss": 0.4435,
      "step": 2280
    },
    {
      "epoch": 1.3574392412566687,
      "grad_norm": 20.649642944335938,
      "learning_rate": 2.7376012645722194e-05,
      "loss": 0.4862,
      "step": 2290
    },
    {
      "epoch": 1.3633669235328987,
      "grad_norm": 9.321807861328125,
      "learning_rate": 2.7277217941118355e-05,
      "loss": 0.4054,
      "step": 2300
    },
    {
      "epoch": 1.3692946058091287,
      "grad_norm": 6.245841979980469,
      "learning_rate": 2.7178423236514523e-05,
      "loss": 0.3897,
      "step": 2310
    },
    {
      "epoch": 1.3752222880853586,
      "grad_norm": 21.49048614501953,
      "learning_rate": 2.7079628531910688e-05,
      "loss": 0.3745,
      "step": 2320
    },
    {
      "epoch": 1.3811499703615886,
      "grad_norm": 23.670412063598633,
      "learning_rate": 2.6980833827306856e-05,
      "loss": 0.5331,
      "step": 2330
    },
    {
      "epoch": 1.3870776526378186,
      "grad_norm": 7.693662166595459,
      "learning_rate": 2.6882039122703023e-05,
      "loss": 0.502,
      "step": 2340
    },
    {
      "epoch": 1.3930053349140485,
      "grad_norm": 5.781563758850098,
      "learning_rate": 2.678324441809919e-05,
      "loss": 0.4203,
      "step": 2350
    },
    {
      "epoch": 1.3989330171902785,
      "grad_norm": 7.678214073181152,
      "learning_rate": 2.6684449713495356e-05,
      "loss": 0.393,
      "step": 2360
    },
    {
      "epoch": 1.4048606994665085,
      "grad_norm": 7.0040435791015625,
      "learning_rate": 2.6585655008891524e-05,
      "loss": 0.4468,
      "step": 2370
    },
    {
      "epoch": 1.4107883817427385,
      "grad_norm": 16.793054580688477,
      "learning_rate": 2.6486860304287692e-05,
      "loss": 0.4883,
      "step": 2380
    },
    {
      "epoch": 1.4167160640189687,
      "grad_norm": 14.5381441116333,
      "learning_rate": 2.638806559968386e-05,
      "loss": 0.3995,
      "step": 2390
    },
    {
      "epoch": 1.4226437462951986,
      "grad_norm": 33.188751220703125,
      "learning_rate": 2.6289270895080024e-05,
      "loss": 0.4006,
      "step": 2400
    },
    {
      "epoch": 1.4285714285714286,
      "grad_norm": 14.136757850646973,
      "learning_rate": 2.6190476190476192e-05,
      "loss": 0.4854,
      "step": 2410
    },
    {
      "epoch": 1.4344991108476586,
      "grad_norm": 46.16522979736328,
      "learning_rate": 2.609168148587236e-05,
      "loss": 0.3553,
      "step": 2420
    },
    {
      "epoch": 1.4404267931238885,
      "grad_norm": 43.665550231933594,
      "learning_rate": 2.5992886781268528e-05,
      "loss": 0.3931,
      "step": 2430
    },
    {
      "epoch": 1.4463544754001185,
      "grad_norm": 9.909360885620117,
      "learning_rate": 2.5894092076664696e-05,
      "loss": 0.488,
      "step": 2440
    },
    {
      "epoch": 1.4522821576763485,
      "grad_norm": 20.598766326904297,
      "learning_rate": 2.579529737206086e-05,
      "loss": 0.4275,
      "step": 2450
    },
    {
      "epoch": 1.4582098399525785,
      "grad_norm": 71.97425842285156,
      "learning_rate": 2.5696502667457022e-05,
      "loss": 0.4483,
      "step": 2460
    },
    {
      "epoch": 1.4641375222288087,
      "grad_norm": 7.717462539672852,
      "learning_rate": 2.559770796285319e-05,
      "loss": 0.4599,
      "step": 2470
    },
    {
      "epoch": 1.4700652045050386,
      "grad_norm": 11.084677696228027,
      "learning_rate": 2.5498913258249358e-05,
      "loss": 0.4729,
      "step": 2480
    },
    {
      "epoch": 1.4759928867812686,
      "grad_norm": 11.593571662902832,
      "learning_rate": 2.5400118553645525e-05,
      "loss": 0.5638,
      "step": 2490
    },
    {
      "epoch": 1.4819205690574986,
      "grad_norm": 41.46894454956055,
      "learning_rate": 2.530132384904169e-05,
      "loss": 0.5486,
      "step": 2500
    },
    {
      "epoch": 1.4878482513337286,
      "grad_norm": 14.49413013458252,
      "learning_rate": 2.5202529144437858e-05,
      "loss": 0.3943,
      "step": 2510
    },
    {
      "epoch": 1.4937759336099585,
      "grad_norm": 4.692091464996338,
      "learning_rate": 2.5103734439834026e-05,
      "loss": 0.377,
      "step": 2520
    },
    {
      "epoch": 1.4997036158861885,
      "grad_norm": 9.429450035095215,
      "learning_rate": 2.5004939735230194e-05,
      "loss": 0.3678,
      "step": 2530
    },
    {
      "epoch": 1.5056312981624185,
      "grad_norm": 771.1930541992188,
      "learning_rate": 2.490614503062636e-05,
      "loss": 0.4061,
      "step": 2540
    },
    {
      "epoch": 1.5115589804386484,
      "grad_norm": 9.893959045410156,
      "learning_rate": 2.4807350326022526e-05,
      "loss": 0.5649,
      "step": 2550
    },
    {
      "epoch": 1.5174866627148784,
      "grad_norm": 10.7116060256958,
      "learning_rate": 2.4708555621418694e-05,
      "loss": 0.5461,
      "step": 2560
    },
    {
      "epoch": 1.5234143449911084,
      "grad_norm": 8.141831398010254,
      "learning_rate": 2.4609760916814862e-05,
      "loss": 0.5039,
      "step": 2570
    },
    {
      "epoch": 1.5293420272673384,
      "grad_norm": 4.466522693634033,
      "learning_rate": 2.4510966212211027e-05,
      "loss": 0.4653,
      "step": 2580
    },
    {
      "epoch": 1.5352697095435683,
      "grad_norm": 14.824300765991211,
      "learning_rate": 2.441217150760719e-05,
      "loss": 0.5454,
      "step": 2590
    },
    {
      "epoch": 1.5411973918197983,
      "grad_norm": 8.726339340209961,
      "learning_rate": 2.431337680300336e-05,
      "loss": 0.5092,
      "step": 2600
    },
    {
      "epoch": 1.5471250740960285,
      "grad_norm": 25.562480926513672,
      "learning_rate": 2.4214582098399527e-05,
      "loss": 0.4882,
      "step": 2610
    },
    {
      "epoch": 1.5530527563722585,
      "grad_norm": 7.9986724853515625,
      "learning_rate": 2.4115787393795695e-05,
      "loss": 0.425,
      "step": 2620
    },
    {
      "epoch": 1.5589804386484885,
      "grad_norm": 72.96670532226562,
      "learning_rate": 2.401699268919186e-05,
      "loss": 0.441,
      "step": 2630
    },
    {
      "epoch": 1.5649081209247184,
      "grad_norm": 29.695537567138672,
      "learning_rate": 2.3918197984588027e-05,
      "loss": 0.3723,
      "step": 2640
    },
    {
      "epoch": 1.5708358032009484,
      "grad_norm": 32.98476791381836,
      "learning_rate": 2.3819403279984192e-05,
      "loss": 0.4276,
      "step": 2650
    },
    {
      "epoch": 1.5767634854771784,
      "grad_norm": 27.10540771484375,
      "learning_rate": 2.372060857538036e-05,
      "loss": 0.4489,
      "step": 2660
    },
    {
      "epoch": 1.5826911677534086,
      "grad_norm": 16.343748092651367,
      "learning_rate": 2.3621813870776528e-05,
      "loss": 0.4793,
      "step": 2670
    },
    {
      "epoch": 1.5886188500296385,
      "grad_norm": 96.24451446533203,
      "learning_rate": 2.3523019166172692e-05,
      "loss": 0.6039,
      "step": 2680
    },
    {
      "epoch": 1.5945465323058685,
      "grad_norm": 19.502498626708984,
      "learning_rate": 2.342422446156886e-05,
      "loss": 0.4297,
      "step": 2690
    },
    {
      "epoch": 1.6004742145820985,
      "grad_norm": 10.080013275146484,
      "learning_rate": 2.3325429756965028e-05,
      "loss": 0.3569,
      "step": 2700
    },
    {
      "epoch": 1.6064018968583285,
      "grad_norm": 25.78598403930664,
      "learning_rate": 2.3226635052361196e-05,
      "loss": 0.4256,
      "step": 2710
    },
    {
      "epoch": 1.6123295791345584,
      "grad_norm": 52.46147155761719,
      "learning_rate": 2.312784034775736e-05,
      "loss": 0.3781,
      "step": 2720
    },
    {
      "epoch": 1.6182572614107884,
      "grad_norm": 8.023353576660156,
      "learning_rate": 2.3029045643153525e-05,
      "loss": 0.3884,
      "step": 2730
    },
    {
      "epoch": 1.6241849436870184,
      "grad_norm": 13.239675521850586,
      "learning_rate": 2.2930250938549693e-05,
      "loss": 0.5072,
      "step": 2740
    },
    {
      "epoch": 1.6301126259632484,
      "grad_norm": 12.868340492248535,
      "learning_rate": 2.283145623394586e-05,
      "loss": 0.4619,
      "step": 2750
    },
    {
      "epoch": 1.6360403082394783,
      "grad_norm": 4.479226112365723,
      "learning_rate": 2.273266152934203e-05,
      "loss": 0.4502,
      "step": 2760
    },
    {
      "epoch": 1.6419679905157083,
      "grad_norm": 8.211589813232422,
      "learning_rate": 2.2633866824738194e-05,
      "loss": 0.4075,
      "step": 2770
    },
    {
      "epoch": 1.6478956727919383,
      "grad_norm": 161.88131713867188,
      "learning_rate": 2.253507212013436e-05,
      "loss": 0.4085,
      "step": 2780
    },
    {
      "epoch": 1.6538233550681682,
      "grad_norm": 27.124574661254883,
      "learning_rate": 2.243627741553053e-05,
      "loss": 0.4312,
      "step": 2790
    },
    {
      "epoch": 1.6597510373443982,
      "grad_norm": 22.21398162841797,
      "learning_rate": 2.2337482710926697e-05,
      "loss": 0.504,
      "step": 2800
    },
    {
      "epoch": 1.6656787196206282,
      "grad_norm": 19.872228622436523,
      "learning_rate": 2.2238688006322862e-05,
      "loss": 0.4947,
      "step": 2810
    },
    {
      "epoch": 1.6716064018968582,
      "grad_norm": 13.635519981384277,
      "learning_rate": 2.2139893301719027e-05,
      "loss": 0.3874,
      "step": 2820
    },
    {
      "epoch": 1.6775340841730884,
      "grad_norm": 16.54168128967285,
      "learning_rate": 2.2041098597115194e-05,
      "loss": 0.3952,
      "step": 2830
    },
    {
      "epoch": 1.6834617664493183,
      "grad_norm": 6.208676338195801,
      "learning_rate": 2.1942303892511362e-05,
      "loss": 0.3959,
      "step": 2840
    },
    {
      "epoch": 1.6893894487255483,
      "grad_norm": 8.01900577545166,
      "learning_rate": 2.184350918790753e-05,
      "loss": 0.479,
      "step": 2850
    },
    {
      "epoch": 1.6953171310017783,
      "grad_norm": 18.013652801513672,
      "learning_rate": 2.1744714483303695e-05,
      "loss": 0.509,
      "step": 2860
    },
    {
      "epoch": 1.7012448132780082,
      "grad_norm": 5.486523151397705,
      "learning_rate": 2.1645919778699863e-05,
      "loss": 0.3947,
      "step": 2870
    },
    {
      "epoch": 1.7071724955542384,
      "grad_norm": 12.441699981689453,
      "learning_rate": 2.154712507409603e-05,
      "loss": 0.4367,
      "step": 2880
    },
    {
      "epoch": 1.7131001778304684,
      "grad_norm": 28.861534118652344,
      "learning_rate": 2.1448330369492195e-05,
      "loss": 0.5846,
      "step": 2890
    },
    {
      "epoch": 1.7190278601066984,
      "grad_norm": 20.043113708496094,
      "learning_rate": 2.1349535664888363e-05,
      "loss": 0.3861,
      "step": 2900
    },
    {
      "epoch": 1.7249555423829284,
      "grad_norm": 16.412654876708984,
      "learning_rate": 2.1250740960284528e-05,
      "loss": 0.393,
      "step": 2910
    },
    {
      "epoch": 1.7308832246591583,
      "grad_norm": 20.56127166748047,
      "learning_rate": 2.1151946255680696e-05,
      "loss": 0.4428,
      "step": 2920
    },
    {
      "epoch": 1.7368109069353883,
      "grad_norm": 8.498906135559082,
      "learning_rate": 2.1053151551076864e-05,
      "loss": 0.3717,
      "step": 2930
    },
    {
      "epoch": 1.7427385892116183,
      "grad_norm": 16.137022018432617,
      "learning_rate": 2.095435684647303e-05,
      "loss": 0.4146,
      "step": 2940
    },
    {
      "epoch": 1.7486662714878483,
      "grad_norm": 11.034566879272461,
      "learning_rate": 2.0855562141869196e-05,
      "loss": 0.4798,
      "step": 2950
    },
    {
      "epoch": 1.7545939537640782,
      "grad_norm": 10.905204772949219,
      "learning_rate": 2.0756767437265364e-05,
      "loss": 0.3836,
      "step": 2960
    },
    {
      "epoch": 1.7605216360403082,
      "grad_norm": 21.20962905883789,
      "learning_rate": 2.065797273266153e-05,
      "loss": 0.3634,
      "step": 2970
    },
    {
      "epoch": 1.7664493183165382,
      "grad_norm": 8.364121437072754,
      "learning_rate": 2.0559178028057696e-05,
      "loss": 0.4191,
      "step": 2980
    },
    {
      "epoch": 1.7723770005927681,
      "grad_norm": 4.935337066650391,
      "learning_rate": 2.0460383323453864e-05,
      "loss": 0.4928,
      "step": 2990
    },
    {
      "epoch": 1.7783046828689981,
      "grad_norm": 16.50261878967285,
      "learning_rate": 2.036158861885003e-05,
      "loss": 0.4291,
      "step": 3000
    },
    {
      "epoch": 1.784232365145228,
      "grad_norm": 12.885981559753418,
      "learning_rate": 2.0262793914246197e-05,
      "loss": 0.4305,
      "step": 3010
    },
    {
      "epoch": 1.790160047421458,
      "grad_norm": 12.017006874084473,
      "learning_rate": 2.0163999209642365e-05,
      "loss": 0.5226,
      "step": 3020
    },
    {
      "epoch": 1.796087729697688,
      "grad_norm": 13.079777717590332,
      "learning_rate": 2.0065204505038533e-05,
      "loss": 0.4446,
      "step": 3030
    },
    {
      "epoch": 1.8020154119739182,
      "grad_norm": 18.528186798095703,
      "learning_rate": 1.9966409800434697e-05,
      "loss": 0.4277,
      "step": 3040
    },
    {
      "epoch": 1.8079430942501482,
      "grad_norm": 8.456416130065918,
      "learning_rate": 1.9867615095830862e-05,
      "loss": 0.4083,
      "step": 3050
    },
    {
      "epoch": 1.8138707765263782,
      "grad_norm": 11.28665542602539,
      "learning_rate": 1.976882039122703e-05,
      "loss": 0.3899,
      "step": 3060
    },
    {
      "epoch": 1.8197984588026082,
      "grad_norm": 9.363496780395508,
      "learning_rate": 1.9670025686623198e-05,
      "loss": 0.3488,
      "step": 3070
    },
    {
      "epoch": 1.8257261410788381,
      "grad_norm": 10.053664207458496,
      "learning_rate": 1.9571230982019366e-05,
      "loss": 0.3847,
      "step": 3080
    },
    {
      "epoch": 1.8316538233550683,
      "grad_norm": 156.98770141601562,
      "learning_rate": 1.947243627741553e-05,
      "loss": 0.4059,
      "step": 3090
    },
    {
      "epoch": 1.8375815056312983,
      "grad_norm": 22.405467987060547,
      "learning_rate": 1.9373641572811698e-05,
      "loss": 0.5201,
      "step": 3100
    },
    {
      "epoch": 1.8435091879075283,
      "grad_norm": 18.752473831176758,
      "learning_rate": 1.9274846868207866e-05,
      "loss": 0.3705,
      "step": 3110
    },
    {
      "epoch": 1.8494368701837582,
      "grad_norm": 12.559650421142578,
      "learning_rate": 1.9176052163604034e-05,
      "loss": 0.2973,
      "step": 3120
    },
    {
      "epoch": 1.8553645524599882,
      "grad_norm": 11.21545696258545,
      "learning_rate": 1.90772574590002e-05,
      "loss": 0.3934,
      "step": 3130
    },
    {
      "epoch": 1.8612922347362182,
      "grad_norm": 7.4538187980651855,
      "learning_rate": 1.8978462754396363e-05,
      "loss": 0.4178,
      "step": 3140
    },
    {
      "epoch": 1.8672199170124482,
      "grad_norm": 8.656839370727539,
      "learning_rate": 1.887966804979253e-05,
      "loss": 0.4839,
      "step": 3150
    },
    {
      "epoch": 1.8731475992886781,
      "grad_norm": 18.967723846435547,
      "learning_rate": 1.87808733451887e-05,
      "loss": 0.4043,
      "step": 3160
    },
    {
      "epoch": 1.879075281564908,
      "grad_norm": 2.652313709259033,
      "learning_rate": 1.8682078640584867e-05,
      "loss": 0.4065,
      "step": 3170
    },
    {
      "epoch": 1.885002963841138,
      "grad_norm": 4.124950885772705,
      "learning_rate": 1.858328393598103e-05,
      "loss": 0.3546,
      "step": 3180
    },
    {
      "epoch": 1.890930646117368,
      "grad_norm": 21.54342269897461,
      "learning_rate": 1.84844892313772e-05,
      "loss": 0.423,
      "step": 3190
    },
    {
      "epoch": 1.896858328393598,
      "grad_norm": 14.04715633392334,
      "learning_rate": 1.8385694526773367e-05,
      "loss": 0.4748,
      "step": 3200
    },
    {
      "epoch": 1.902786010669828,
      "grad_norm": 11.701040267944336,
      "learning_rate": 1.8286899822169532e-05,
      "loss": 0.3863,
      "step": 3210
    },
    {
      "epoch": 1.908713692946058,
      "grad_norm": 128.43197631835938,
      "learning_rate": 1.81881051175657e-05,
      "loss": 0.3644,
      "step": 3220
    },
    {
      "epoch": 1.914641375222288,
      "grad_norm": 11.4481201171875,
      "learning_rate": 1.8089310412961864e-05,
      "loss": 0.3906,
      "step": 3230
    },
    {
      "epoch": 1.920569057498518,
      "grad_norm": 16.677833557128906,
      "learning_rate": 1.7990515708358032e-05,
      "loss": 0.4422,
      "step": 3240
    },
    {
      "epoch": 1.9264967397747481,
      "grad_norm": 17.389015197753906,
      "learning_rate": 1.78917210037542e-05,
      "loss": 0.4421,
      "step": 3250
    },
    {
      "epoch": 1.932424422050978,
      "grad_norm": 13.845840454101562,
      "learning_rate": 1.7792926299150368e-05,
      "loss": 0.3419,
      "step": 3260
    },
    {
      "epoch": 1.938352104327208,
      "grad_norm": 37.173583984375,
      "learning_rate": 1.7694131594546533e-05,
      "loss": 0.4121,
      "step": 3270
    },
    {
      "epoch": 1.944279786603438,
      "grad_norm": 15.189359664916992,
      "learning_rate": 1.75953368899427e-05,
      "loss": 0.5034,
      "step": 3280
    },
    {
      "epoch": 1.950207468879668,
      "grad_norm": 5.964058876037598,
      "learning_rate": 1.7496542185338865e-05,
      "loss": 0.3884,
      "step": 3290
    },
    {
      "epoch": 1.9561351511558982,
      "grad_norm": 26.13176727294922,
      "learning_rate": 1.7397747480735033e-05,
      "loss": 0.3874,
      "step": 3300
    },
    {
      "epoch": 1.9620628334321282,
      "grad_norm": 14.379884719848633,
      "learning_rate": 1.72989527761312e-05,
      "loss": 0.4537,
      "step": 3310
    },
    {
      "epoch": 1.9679905157083581,
      "grad_norm": 13.412887573242188,
      "learning_rate": 1.7200158071527365e-05,
      "loss": 0.3937,
      "step": 3320
    },
    {
      "epoch": 1.9739181979845881,
      "grad_norm": 15.514087677001953,
      "learning_rate": 1.7101363366923533e-05,
      "loss": 0.47,
      "step": 3330
    },
    {
      "epoch": 1.979845880260818,
      "grad_norm": 106.9461898803711,
      "learning_rate": 1.70025686623197e-05,
      "loss": 0.4293,
      "step": 3340
    },
    {
      "epoch": 1.985773562537048,
      "grad_norm": 5.95479679107666,
      "learning_rate": 1.690377395771587e-05,
      "loss": 0.4253,
      "step": 3350
    },
    {
      "epoch": 1.991701244813278,
      "grad_norm": 16.082477569580078,
      "learning_rate": 1.6804979253112034e-05,
      "loss": 0.3308,
      "step": 3360
    },
    {
      "epoch": 1.997628927089508,
      "grad_norm": 5.0378594398498535,
      "learning_rate": 1.6706184548508198e-05,
      "loss": 0.3061,
      "step": 3370
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.8071449747998815,
      "eval_f1": 0.7688754663350507,
      "eval_loss": 0.4250943958759308,
      "eval_precision": 0.8228136882129278,
      "eval_recall": 0.7215738579526508,
      "eval_runtime": 13.3946,
      "eval_samples_per_second": 503.637,
      "eval_steps_per_second": 31.505,
      "step": 3374
    }
  ],
  "logging_steps": 10,
  "max_steps": 5061,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 322421239895040.0,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
