{
  "best_metric": 0.4941551387310028,
  "best_model_checkpoint": "c:\\Users\\chuvi\\Code\\SUCode\\ANLP\\anlp-project\\anlp-project\\AutoDep_Master\\results\\text\\albert-base-v2\\checkpoint-1687",
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 1687,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.005927682276229994,
      "grad_norm": 10.161124229431152,
      "learning_rate": 4.990120529539617e-05,
      "loss": 0.6908,
      "step": 10
    },
    {
      "epoch": 0.011855364552459988,
      "grad_norm": 13.26170825958252,
      "learning_rate": 4.980241059079234e-05,
      "loss": 0.6408,
      "step": 20
    },
    {
      "epoch": 0.01778304682868998,
      "grad_norm": 15.047383308410645,
      "learning_rate": 4.9703615886188506e-05,
      "loss": 0.6749,
      "step": 30
    },
    {
      "epoch": 0.023710729104919975,
      "grad_norm": 11.168242454528809,
      "learning_rate": 4.9604821181584674e-05,
      "loss": 0.6578,
      "step": 40
    },
    {
      "epoch": 0.02963841138114997,
      "grad_norm": 16.361614227294922,
      "learning_rate": 4.9506026476980835e-05,
      "loss": 0.6153,
      "step": 50
    },
    {
      "epoch": 0.03556609365737996,
      "grad_norm": 10.466825485229492,
      "learning_rate": 4.9407231772377e-05,
      "loss": 0.6618,
      "step": 60
    },
    {
      "epoch": 0.04149377593360996,
      "grad_norm": 20.217893600463867,
      "learning_rate": 4.930843706777317e-05,
      "loss": 0.6304,
      "step": 70
    },
    {
      "epoch": 0.04742145820983995,
      "grad_norm": 13.51468563079834,
      "learning_rate": 4.920964236316933e-05,
      "loss": 0.5592,
      "step": 80
    },
    {
      "epoch": 0.053349140486069944,
      "grad_norm": 9.34771728515625,
      "learning_rate": 4.91108476585655e-05,
      "loss": 0.5899,
      "step": 90
    },
    {
      "epoch": 0.05927682276229994,
      "grad_norm": 42.417335510253906,
      "learning_rate": 4.901205295396167e-05,
      "loss": 0.6186,
      "step": 100
    },
    {
      "epoch": 0.06520450503852994,
      "grad_norm": 15.10250473022461,
      "learning_rate": 4.8913258249357836e-05,
      "loss": 0.6338,
      "step": 110
    },
    {
      "epoch": 0.07113218731475993,
      "grad_norm": 250.51513671875,
      "learning_rate": 4.8814463544754004e-05,
      "loss": 0.6145,
      "step": 120
    },
    {
      "epoch": 0.07705986959098993,
      "grad_norm": 42.902496337890625,
      "learning_rate": 4.871566884015017e-05,
      "loss": 0.6367,
      "step": 130
    },
    {
      "epoch": 0.08298755186721991,
      "grad_norm": 10.545756340026855,
      "learning_rate": 4.861687413554633e-05,
      "loss": 0.6419,
      "step": 140
    },
    {
      "epoch": 0.08891523414344991,
      "grad_norm": 26.169902801513672,
      "learning_rate": 4.85180794309425e-05,
      "loss": 0.5338,
      "step": 150
    },
    {
      "epoch": 0.0948429164196799,
      "grad_norm": 19.757549285888672,
      "learning_rate": 4.841928472633867e-05,
      "loss": 0.6635,
      "step": 160
    },
    {
      "epoch": 0.1007705986959099,
      "grad_norm": 15.494487762451172,
      "learning_rate": 4.832049002173484e-05,
      "loss": 0.5874,
      "step": 170
    },
    {
      "epoch": 0.10669828097213989,
      "grad_norm": 16.69301986694336,
      "learning_rate": 4.8221695317131005e-05,
      "loss": 0.5775,
      "step": 180
    },
    {
      "epoch": 0.11262596324836989,
      "grad_norm": 7.297093391418457,
      "learning_rate": 4.812290061252717e-05,
      "loss": 0.5198,
      "step": 190
    },
    {
      "epoch": 0.11855364552459988,
      "grad_norm": 33.785987854003906,
      "learning_rate": 4.802410590792334e-05,
      "loss": 0.6286,
      "step": 200
    },
    {
      "epoch": 0.12448132780082988,
      "grad_norm": 9.313821792602539,
      "learning_rate": 4.792531120331951e-05,
      "loss": 0.6568,
      "step": 210
    },
    {
      "epoch": 0.13040901007705988,
      "grad_norm": 11.622394561767578,
      "learning_rate": 4.7826516498715677e-05,
      "loss": 0.6086,
      "step": 220
    },
    {
      "epoch": 0.13633669235328985,
      "grad_norm": 15.57612419128418,
      "learning_rate": 4.772772179411184e-05,
      "loss": 0.576,
      "step": 230
    },
    {
      "epoch": 0.14226437462951985,
      "grad_norm": 6.794140815734863,
      "learning_rate": 4.7628927089508e-05,
      "loss": 0.5607,
      "step": 240
    },
    {
      "epoch": 0.14819205690574985,
      "grad_norm": 14.009489059448242,
      "learning_rate": 4.753013238490417e-05,
      "loss": 0.5411,
      "step": 250
    },
    {
      "epoch": 0.15411973918197985,
      "grad_norm": 11.607579231262207,
      "learning_rate": 4.7431337680300335e-05,
      "loss": 0.5558,
      "step": 260
    },
    {
      "epoch": 0.16004742145820983,
      "grad_norm": 10.619332313537598,
      "learning_rate": 4.73325429756965e-05,
      "loss": 0.6315,
      "step": 270
    },
    {
      "epoch": 0.16597510373443983,
      "grad_norm": 5.460982322692871,
      "learning_rate": 4.723374827109267e-05,
      "loss": 0.6012,
      "step": 280
    },
    {
      "epoch": 0.17190278601066983,
      "grad_norm": 8.422821998596191,
      "learning_rate": 4.713495356648884e-05,
      "loss": 0.5598,
      "step": 290
    },
    {
      "epoch": 0.17783046828689983,
      "grad_norm": 6.931854248046875,
      "learning_rate": 4.7036158861885006e-05,
      "loss": 0.5793,
      "step": 300
    },
    {
      "epoch": 0.18375815056312983,
      "grad_norm": 20.177288055419922,
      "learning_rate": 4.6937364157281174e-05,
      "loss": 0.4974,
      "step": 310
    },
    {
      "epoch": 0.1896858328393598,
      "grad_norm": 9.56718635559082,
      "learning_rate": 4.6838569452677335e-05,
      "loss": 0.5469,
      "step": 320
    },
    {
      "epoch": 0.1956135151155898,
      "grad_norm": 68.67320251464844,
      "learning_rate": 4.6739774748073503e-05,
      "loss": 0.5815,
      "step": 330
    },
    {
      "epoch": 0.2015411973918198,
      "grad_norm": 122.79833221435547,
      "learning_rate": 4.664098004346967e-05,
      "loss": 0.5245,
      "step": 340
    },
    {
      "epoch": 0.2074688796680498,
      "grad_norm": 78.1559829711914,
      "learning_rate": 4.654218533886584e-05,
      "loss": 0.6321,
      "step": 350
    },
    {
      "epoch": 0.21339656194427978,
      "grad_norm": 24.242849349975586,
      "learning_rate": 4.644339063426201e-05,
      "loss": 0.5814,
      "step": 360
    },
    {
      "epoch": 0.21932424422050978,
      "grad_norm": 36.89963912963867,
      "learning_rate": 4.6344595929658175e-05,
      "loss": 0.651,
      "step": 370
    },
    {
      "epoch": 0.22525192649673978,
      "grad_norm": 20.859704971313477,
      "learning_rate": 4.624580122505434e-05,
      "loss": 0.6346,
      "step": 380
    },
    {
      "epoch": 0.23117960877296978,
      "grad_norm": 24.917713165283203,
      "learning_rate": 4.614700652045051e-05,
      "loss": 0.6302,
      "step": 390
    },
    {
      "epoch": 0.23710729104919975,
      "grad_norm": 29.016895294189453,
      "learning_rate": 4.604821181584667e-05,
      "loss": 0.5671,
      "step": 400
    },
    {
      "epoch": 0.24303497332542975,
      "grad_norm": 12.832783699035645,
      "learning_rate": 4.594941711124284e-05,
      "loss": 0.5664,
      "step": 410
    },
    {
      "epoch": 0.24896265560165975,
      "grad_norm": 11.57109546661377,
      "learning_rate": 4.5850622406639e-05,
      "loss": 0.5259,
      "step": 420
    },
    {
      "epoch": 0.2548903378778897,
      "grad_norm": 14.475279808044434,
      "learning_rate": 4.575182770203517e-05,
      "loss": 0.5819,
      "step": 430
    },
    {
      "epoch": 0.26081802015411976,
      "grad_norm": 63.90249252319336,
      "learning_rate": 4.565303299743134e-05,
      "loss": 0.6211,
      "step": 440
    },
    {
      "epoch": 0.26674570243034973,
      "grad_norm": 73.16571044921875,
      "learning_rate": 4.5554238292827505e-05,
      "loss": 0.5551,
      "step": 450
    },
    {
      "epoch": 0.2726733847065797,
      "grad_norm": 36.59919357299805,
      "learning_rate": 4.545544358822367e-05,
      "loss": 0.5478,
      "step": 460
    },
    {
      "epoch": 0.27860106698280973,
      "grad_norm": 27.131576538085938,
      "learning_rate": 4.535664888361984e-05,
      "loss": 0.6151,
      "step": 470
    },
    {
      "epoch": 0.2845287492590397,
      "grad_norm": 14.471365928649902,
      "learning_rate": 4.525785417901601e-05,
      "loss": 0.5712,
      "step": 480
    },
    {
      "epoch": 0.29045643153526973,
      "grad_norm": 13.836601257324219,
      "learning_rate": 4.515905947441218e-05,
      "loss": 0.5986,
      "step": 490
    },
    {
      "epoch": 0.2963841138114997,
      "grad_norm": 9.851883888244629,
      "learning_rate": 4.5060264769808345e-05,
      "loss": 0.6126,
      "step": 500
    },
    {
      "epoch": 0.3023117960877297,
      "grad_norm": 8.354625701904297,
      "learning_rate": 4.4961470065204506e-05,
      "loss": 0.521,
      "step": 510
    },
    {
      "epoch": 0.3082394783639597,
      "grad_norm": 19.55716323852539,
      "learning_rate": 4.4862675360600674e-05,
      "loss": 0.5017,
      "step": 520
    },
    {
      "epoch": 0.3141671606401897,
      "grad_norm": 17.373065948486328,
      "learning_rate": 4.476388065599684e-05,
      "loss": 0.4918,
      "step": 530
    },
    {
      "epoch": 0.32009484291641965,
      "grad_norm": 12.192534446716309,
      "learning_rate": 4.466508595139301e-05,
      "loss": 0.5234,
      "step": 540
    },
    {
      "epoch": 0.3260225251926497,
      "grad_norm": 21.954313278198242,
      "learning_rate": 4.456629124678918e-05,
      "loss": 0.5151,
      "step": 550
    },
    {
      "epoch": 0.33195020746887965,
      "grad_norm": 16.208221435546875,
      "learning_rate": 4.446749654218534e-05,
      "loss": 0.5937,
      "step": 560
    },
    {
      "epoch": 0.3378778897451097,
      "grad_norm": 55.068504333496094,
      "learning_rate": 4.436870183758151e-05,
      "loss": 0.5697,
      "step": 570
    },
    {
      "epoch": 0.34380557202133966,
      "grad_norm": 45.51179122924805,
      "learning_rate": 4.4269907132977675e-05,
      "loss": 0.5129,
      "step": 580
    },
    {
      "epoch": 0.34973325429756963,
      "grad_norm": 12.437984466552734,
      "learning_rate": 4.417111242837384e-05,
      "loss": 0.6185,
      "step": 590
    },
    {
      "epoch": 0.35566093657379966,
      "grad_norm": 18.581085205078125,
      "learning_rate": 4.4072317723770004e-05,
      "loss": 0.5165,
      "step": 600
    },
    {
      "epoch": 0.36158861885002963,
      "grad_norm": 23.36187744140625,
      "learning_rate": 4.397352301916617e-05,
      "loss": 0.5115,
      "step": 610
    },
    {
      "epoch": 0.36751630112625966,
      "grad_norm": 13.504555702209473,
      "learning_rate": 4.387472831456234e-05,
      "loss": 0.5639,
      "step": 620
    },
    {
      "epoch": 0.37344398340248963,
      "grad_norm": 31.79749870300293,
      "learning_rate": 4.377593360995851e-05,
      "loss": 0.5043,
      "step": 630
    },
    {
      "epoch": 0.3793716656787196,
      "grad_norm": 38.95268249511719,
      "learning_rate": 4.3677138905354675e-05,
      "loss": 0.5202,
      "step": 640
    },
    {
      "epoch": 0.38529934795494963,
      "grad_norm": 24.532787322998047,
      "learning_rate": 4.357834420075084e-05,
      "loss": 0.4589,
      "step": 650
    },
    {
      "epoch": 0.3912270302311796,
      "grad_norm": 12.012605667114258,
      "learning_rate": 4.347954949614701e-05,
      "loss": 0.5539,
      "step": 660
    },
    {
      "epoch": 0.3971547125074096,
      "grad_norm": 11.958657264709473,
      "learning_rate": 4.338075479154318e-05,
      "loss": 0.4616,
      "step": 670
    },
    {
      "epoch": 0.4030823947836396,
      "grad_norm": 40.62808609008789,
      "learning_rate": 4.328196008693935e-05,
      "loss": 0.5088,
      "step": 680
    },
    {
      "epoch": 0.4090100770598696,
      "grad_norm": 47.91569900512695,
      "learning_rate": 4.318316538233551e-05,
      "loss": 0.5183,
      "step": 690
    },
    {
      "epoch": 0.4149377593360996,
      "grad_norm": 16.3569278717041,
      "learning_rate": 4.3084370677731676e-05,
      "loss": 0.5828,
      "step": 700
    },
    {
      "epoch": 0.4208654416123296,
      "grad_norm": 47.268375396728516,
      "learning_rate": 4.2985575973127844e-05,
      "loss": 0.4905,
      "step": 710
    },
    {
      "epoch": 0.42679312388855956,
      "grad_norm": 71.96983337402344,
      "learning_rate": 4.2886781268524005e-05,
      "loss": 0.5331,
      "step": 720
    },
    {
      "epoch": 0.4327208061647896,
      "grad_norm": 5.641787528991699,
      "learning_rate": 4.278798656392017e-05,
      "loss": 0.6428,
      "step": 730
    },
    {
      "epoch": 0.43864848844101956,
      "grad_norm": 51.863365173339844,
      "learning_rate": 4.268919185931634e-05,
      "loss": 0.4849,
      "step": 740
    },
    {
      "epoch": 0.44457617071724953,
      "grad_norm": 37.18937301635742,
      "learning_rate": 4.259039715471251e-05,
      "loss": 0.5447,
      "step": 750
    },
    {
      "epoch": 0.45050385299347956,
      "grad_norm": 11.722724914550781,
      "learning_rate": 4.249160245010868e-05,
      "loss": 0.5476,
      "step": 760
    },
    {
      "epoch": 0.45643153526970953,
      "grad_norm": 43.98824691772461,
      "learning_rate": 4.2392807745504845e-05,
      "loss": 0.6262,
      "step": 770
    },
    {
      "epoch": 0.46235921754593956,
      "grad_norm": 8.402789115905762,
      "learning_rate": 4.2294013040901006e-05,
      "loss": 0.5162,
      "step": 780
    },
    {
      "epoch": 0.46828689982216953,
      "grad_norm": 33.1689567565918,
      "learning_rate": 4.2195218336297174e-05,
      "loss": 0.5358,
      "step": 790
    },
    {
      "epoch": 0.4742145820983995,
      "grad_norm": 15.662578582763672,
      "learning_rate": 4.209642363169334e-05,
      "loss": 0.4971,
      "step": 800
    },
    {
      "epoch": 0.48014226437462953,
      "grad_norm": 32.71614074707031,
      "learning_rate": 4.199762892708951e-05,
      "loss": 0.4791,
      "step": 810
    },
    {
      "epoch": 0.4860699466508595,
      "grad_norm": 13.527876853942871,
      "learning_rate": 4.189883422248568e-05,
      "loss": 0.4655,
      "step": 820
    },
    {
      "epoch": 0.4919976289270895,
      "grad_norm": 8.002348899841309,
      "learning_rate": 4.1800039517881846e-05,
      "loss": 0.5425,
      "step": 830
    },
    {
      "epoch": 0.4979253112033195,
      "grad_norm": 42.055450439453125,
      "learning_rate": 4.1701244813278014e-05,
      "loss": 0.5944,
      "step": 840
    },
    {
      "epoch": 0.5038529934795495,
      "grad_norm": 6.069934368133545,
      "learning_rate": 4.160245010867418e-05,
      "loss": 0.6239,
      "step": 850
    },
    {
      "epoch": 0.5097806757557795,
      "grad_norm": 6.783214569091797,
      "learning_rate": 4.150365540407035e-05,
      "loss": 0.65,
      "step": 860
    },
    {
      "epoch": 0.5157083580320095,
      "grad_norm": 75.57958221435547,
      "learning_rate": 4.140486069946651e-05,
      "loss": 0.4999,
      "step": 870
    },
    {
      "epoch": 0.5216360403082395,
      "grad_norm": 14.70267105102539,
      "learning_rate": 4.130606599486267e-05,
      "loss": 0.5237,
      "step": 880
    },
    {
      "epoch": 0.5275637225844695,
      "grad_norm": 49.0606803894043,
      "learning_rate": 4.120727129025884e-05,
      "loss": 0.5604,
      "step": 890
    },
    {
      "epoch": 0.5334914048606995,
      "grad_norm": 5.73143196105957,
      "learning_rate": 4.110847658565501e-05,
      "loss": 0.5571,
      "step": 900
    },
    {
      "epoch": 0.5394190871369294,
      "grad_norm": 8.64951229095459,
      "learning_rate": 4.1009681881051176e-05,
      "loss": 0.5124,
      "step": 910
    },
    {
      "epoch": 0.5453467694131594,
      "grad_norm": 25.14156150817871,
      "learning_rate": 4.0910887176447343e-05,
      "loss": 0.6028,
      "step": 920
    },
    {
      "epoch": 0.5512744516893895,
      "grad_norm": 6.401345729827881,
      "learning_rate": 4.081209247184351e-05,
      "loss": 0.5774,
      "step": 930
    },
    {
      "epoch": 0.5572021339656195,
      "grad_norm": 8.712220191955566,
      "learning_rate": 4.071329776723968e-05,
      "loss": 0.5753,
      "step": 940
    },
    {
      "epoch": 0.5631298162418494,
      "grad_norm": 11.452255249023438,
      "learning_rate": 4.061450306263585e-05,
      "loss": 0.5115,
      "step": 950
    },
    {
      "epoch": 0.5690574985180794,
      "grad_norm": 10.3247652053833,
      "learning_rate": 4.051570835803201e-05,
      "loss": 0.457,
      "step": 960
    },
    {
      "epoch": 0.5749851807943094,
      "grad_norm": 8.051189422607422,
      "learning_rate": 4.0416913653428176e-05,
      "loss": 0.6535,
      "step": 970
    },
    {
      "epoch": 0.5809128630705395,
      "grad_norm": 7.823803901672363,
      "learning_rate": 4.0318118948824344e-05,
      "loss": 0.5654,
      "step": 980
    },
    {
      "epoch": 0.5868405453467694,
      "grad_norm": 2.754333019256592,
      "learning_rate": 4.021932424422051e-05,
      "loss": 0.561,
      "step": 990
    },
    {
      "epoch": 0.5927682276229994,
      "grad_norm": 3.2010560035705566,
      "learning_rate": 4.012052953961668e-05,
      "loss": 0.6191,
      "step": 1000
    },
    {
      "epoch": 0.5986959098992294,
      "grad_norm": 20.963420867919922,
      "learning_rate": 4.002173483501285e-05,
      "loss": 0.5632,
      "step": 1010
    },
    {
      "epoch": 0.6046235921754594,
      "grad_norm": 7.515842437744141,
      "learning_rate": 3.9922940130409016e-05,
      "loss": 0.561,
      "step": 1020
    },
    {
      "epoch": 0.6105512744516894,
      "grad_norm": 58.44517517089844,
      "learning_rate": 3.982414542580518e-05,
      "loss": 0.5497,
      "step": 1030
    },
    {
      "epoch": 0.6164789567279194,
      "grad_norm": 18.94573211669922,
      "learning_rate": 3.9725350721201345e-05,
      "loss": 0.4113,
      "step": 1040
    },
    {
      "epoch": 0.6224066390041494,
      "grad_norm": 238.8856964111328,
      "learning_rate": 3.962655601659751e-05,
      "loss": 0.518,
      "step": 1050
    },
    {
      "epoch": 0.6283343212803794,
      "grad_norm": 101.71446990966797,
      "learning_rate": 3.9527761311993674e-05,
      "loss": 0.5327,
      "step": 1060
    },
    {
      "epoch": 0.6342620035566093,
      "grad_norm": 6.219387054443359,
      "learning_rate": 3.942896660738984e-05,
      "loss": 0.5326,
      "step": 1070
    },
    {
      "epoch": 0.6401896858328393,
      "grad_norm": 5.771879196166992,
      "learning_rate": 3.933017190278601e-05,
      "loss": 0.4716,
      "step": 1080
    },
    {
      "epoch": 0.6461173681090694,
      "grad_norm": 32.685272216796875,
      "learning_rate": 3.923137719818218e-05,
      "loss": 0.5534,
      "step": 1090
    },
    {
      "epoch": 0.6520450503852994,
      "grad_norm": 19.75934410095215,
      "learning_rate": 3.9132582493578346e-05,
      "loss": 0.4991,
      "step": 1100
    },
    {
      "epoch": 0.6579727326615293,
      "grad_norm": 17.99289894104004,
      "learning_rate": 3.9033787788974514e-05,
      "loss": 0.576,
      "step": 1110
    },
    {
      "epoch": 0.6639004149377593,
      "grad_norm": 38.65797805786133,
      "learning_rate": 3.893499308437068e-05,
      "loss": 0.4983,
      "step": 1120
    },
    {
      "epoch": 0.6698280972139893,
      "grad_norm": 6.34208345413208,
      "learning_rate": 3.883619837976685e-05,
      "loss": 0.4788,
      "step": 1130
    },
    {
      "epoch": 0.6757557794902194,
      "grad_norm": 16.977649688720703,
      "learning_rate": 3.873740367516302e-05,
      "loss": 0.4308,
      "step": 1140
    },
    {
      "epoch": 0.6816834617664493,
      "grad_norm": 10.427937507629395,
      "learning_rate": 3.863860897055918e-05,
      "loss": 0.5163,
      "step": 1150
    },
    {
      "epoch": 0.6876111440426793,
      "grad_norm": 228.77536010742188,
      "learning_rate": 3.853981426595535e-05,
      "loss": 0.5197,
      "step": 1160
    },
    {
      "epoch": 0.6935388263189093,
      "grad_norm": 12.161484718322754,
      "learning_rate": 3.8441019561351515e-05,
      "loss": 0.5464,
      "step": 1170
    },
    {
      "epoch": 0.6994665085951393,
      "grad_norm": 6.431323528289795,
      "learning_rate": 3.834222485674768e-05,
      "loss": 0.457,
      "step": 1180
    },
    {
      "epoch": 0.7053941908713693,
      "grad_norm": 4.493706226348877,
      "learning_rate": 3.8243430152143844e-05,
      "loss": 0.6127,
      "step": 1190
    },
    {
      "epoch": 0.7113218731475993,
      "grad_norm": 104.3836441040039,
      "learning_rate": 3.814463544754001e-05,
      "loss": 0.5101,
      "step": 1200
    },
    {
      "epoch": 0.7172495554238293,
      "grad_norm": 5.14698600769043,
      "learning_rate": 3.804584074293618e-05,
      "loss": 0.4577,
      "step": 1210
    },
    {
      "epoch": 0.7231772377000593,
      "grad_norm": 23.63407325744629,
      "learning_rate": 3.794704603833235e-05,
      "loss": 0.4613,
      "step": 1220
    },
    {
      "epoch": 0.7291049199762892,
      "grad_norm": 11.728093147277832,
      "learning_rate": 3.7848251333728515e-05,
      "loss": 0.4911,
      "step": 1230
    },
    {
      "epoch": 0.7350326022525193,
      "grad_norm": 45.91849899291992,
      "learning_rate": 3.7749456629124677e-05,
      "loss": 0.485,
      "step": 1240
    },
    {
      "epoch": 0.7409602845287493,
      "grad_norm": 12.272936820983887,
      "learning_rate": 3.7650661924520845e-05,
      "loss": 0.468,
      "step": 1250
    },
    {
      "epoch": 0.7468879668049793,
      "grad_norm": 18.5985050201416,
      "learning_rate": 3.755186721991701e-05,
      "loss": 0.5322,
      "step": 1260
    },
    {
      "epoch": 0.7528156490812092,
      "grad_norm": 29.861644744873047,
      "learning_rate": 3.745307251531318e-05,
      "loss": 0.5181,
      "step": 1270
    },
    {
      "epoch": 0.7587433313574392,
      "grad_norm": 13.890777587890625,
      "learning_rate": 3.735427781070935e-05,
      "loss": 0.4041,
      "step": 1280
    },
    {
      "epoch": 0.7646710136336692,
      "grad_norm": 48.92064666748047,
      "learning_rate": 3.7255483106105516e-05,
      "loss": 0.5757,
      "step": 1290
    },
    {
      "epoch": 0.7705986959098993,
      "grad_norm": 11.369391441345215,
      "learning_rate": 3.7156688401501684e-05,
      "loss": 0.5061,
      "step": 1300
    },
    {
      "epoch": 0.7765263781861292,
      "grad_norm": 4.402633190155029,
      "learning_rate": 3.705789369689785e-05,
      "loss": 0.5023,
      "step": 1310
    },
    {
      "epoch": 0.7824540604623592,
      "grad_norm": 31.41122055053711,
      "learning_rate": 3.695909899229402e-05,
      "loss": 0.5182,
      "step": 1320
    },
    {
      "epoch": 0.7883817427385892,
      "grad_norm": 16.837535858154297,
      "learning_rate": 3.686030428769018e-05,
      "loss": 0.6012,
      "step": 1330
    },
    {
      "epoch": 0.7943094250148192,
      "grad_norm": 17.68408966064453,
      "learning_rate": 3.676150958308635e-05,
      "loss": 0.5181,
      "step": 1340
    },
    {
      "epoch": 0.8002371072910492,
      "grad_norm": 28.281538009643555,
      "learning_rate": 3.666271487848251e-05,
      "loss": 0.5188,
      "step": 1350
    },
    {
      "epoch": 0.8061647895672792,
      "grad_norm": 9.940011024475098,
      "learning_rate": 3.656392017387868e-05,
      "loss": 0.4345,
      "step": 1360
    },
    {
      "epoch": 0.8120924718435092,
      "grad_norm": 293.8431701660156,
      "learning_rate": 3.6465125469274846e-05,
      "loss": 0.4613,
      "step": 1370
    },
    {
      "epoch": 0.8180201541197392,
      "grad_norm": 20.543962478637695,
      "learning_rate": 3.6366330764671014e-05,
      "loss": 0.4853,
      "step": 1380
    },
    {
      "epoch": 0.8239478363959691,
      "grad_norm": 29.52580451965332,
      "learning_rate": 3.626753606006718e-05,
      "loss": 0.5348,
      "step": 1390
    },
    {
      "epoch": 0.8298755186721992,
      "grad_norm": 87.13895416259766,
      "learning_rate": 3.616874135546335e-05,
      "loss": 0.4216,
      "step": 1400
    },
    {
      "epoch": 0.8358032009484292,
      "grad_norm": 7.074516773223877,
      "learning_rate": 3.606994665085952e-05,
      "loss": 0.5355,
      "step": 1410
    },
    {
      "epoch": 0.8417308832246592,
      "grad_norm": 323.1661376953125,
      "learning_rate": 3.597115194625568e-05,
      "loss": 0.4674,
      "step": 1420
    },
    {
      "epoch": 0.8476585655008891,
      "grad_norm": 11.765792846679688,
      "learning_rate": 3.587235724165185e-05,
      "loss": 0.5105,
      "step": 1430
    },
    {
      "epoch": 0.8535862477771191,
      "grad_norm": 13.943761825561523,
      "learning_rate": 3.5773562537048015e-05,
      "loss": 0.506,
      "step": 1440
    },
    {
      "epoch": 0.8595139300533492,
      "grad_norm": 7.428855895996094,
      "learning_rate": 3.567476783244418e-05,
      "loss": 0.5411,
      "step": 1450
    },
    {
      "epoch": 0.8654416123295792,
      "grad_norm": 7.060905456542969,
      "learning_rate": 3.557597312784035e-05,
      "loss": 0.4916,
      "step": 1460
    },
    {
      "epoch": 0.8713692946058091,
      "grad_norm": 6.235435485839844,
      "learning_rate": 3.547717842323652e-05,
      "loss": 0.3408,
      "step": 1470
    },
    {
      "epoch": 0.8772969768820391,
      "grad_norm": 12.985630989074707,
      "learning_rate": 3.5378383718632687e-05,
      "loss": 0.5413,
      "step": 1480
    },
    {
      "epoch": 0.8832246591582691,
      "grad_norm": 16.37630844116211,
      "learning_rate": 3.5279589014028855e-05,
      "loss": 0.5489,
      "step": 1490
    },
    {
      "epoch": 0.8891523414344991,
      "grad_norm": 6.321455001831055,
      "learning_rate": 3.518079430942502e-05,
      "loss": 0.4749,
      "step": 1500
    },
    {
      "epoch": 0.8950800237107291,
      "grad_norm": 164.0301055908203,
      "learning_rate": 3.5081999604821184e-05,
      "loss": 0.4344,
      "step": 1510
    },
    {
      "epoch": 0.9010077059869591,
      "grad_norm": 5.860137939453125,
      "learning_rate": 3.4983204900217345e-05,
      "loss": 0.4825,
      "step": 1520
    },
    {
      "epoch": 0.9069353882631891,
      "grad_norm": 6.089977741241455,
      "learning_rate": 3.488441019561351e-05,
      "loss": 0.4533,
      "step": 1530
    },
    {
      "epoch": 0.9128630705394191,
      "grad_norm": 24.377971649169922,
      "learning_rate": 3.478561549100968e-05,
      "loss": 0.4752,
      "step": 1540
    },
    {
      "epoch": 0.918790752815649,
      "grad_norm": 20.785724639892578,
      "learning_rate": 3.468682078640585e-05,
      "loss": 0.5295,
      "step": 1550
    },
    {
      "epoch": 0.9247184350918791,
      "grad_norm": 57.24374771118164,
      "learning_rate": 3.4588026081802016e-05,
      "loss": 0.7653,
      "step": 1560
    },
    {
      "epoch": 0.9306461173681091,
      "grad_norm": 5.909299850463867,
      "learning_rate": 3.4489231377198184e-05,
      "loss": 0.5072,
      "step": 1570
    },
    {
      "epoch": 0.9365737996443391,
      "grad_norm": 745.7210083007812,
      "learning_rate": 3.439043667259435e-05,
      "loss": 0.4866,
      "step": 1580
    },
    {
      "epoch": 0.942501481920569,
      "grad_norm": 7.1468329429626465,
      "learning_rate": 3.429164196799052e-05,
      "loss": 0.4974,
      "step": 1590
    },
    {
      "epoch": 0.948429164196799,
      "grad_norm": 8.773387908935547,
      "learning_rate": 3.419284726338668e-05,
      "loss": 0.5335,
      "step": 1600
    },
    {
      "epoch": 0.9543568464730291,
      "grad_norm": 13.26337718963623,
      "learning_rate": 3.409405255878285e-05,
      "loss": 0.4622,
      "step": 1610
    },
    {
      "epoch": 0.9602845287492591,
      "grad_norm": 8.9894380569458,
      "learning_rate": 3.399525785417902e-05,
      "loss": 0.5418,
      "step": 1620
    },
    {
      "epoch": 0.966212211025489,
      "grad_norm": 21.272056579589844,
      "learning_rate": 3.3896463149575185e-05,
      "loss": 0.5654,
      "step": 1630
    },
    {
      "epoch": 0.972139893301719,
      "grad_norm": 6.448025226593018,
      "learning_rate": 3.379766844497135e-05,
      "loss": 0.5242,
      "step": 1640
    },
    {
      "epoch": 0.978067575577949,
      "grad_norm": 6.166467189788818,
      "learning_rate": 3.369887374036752e-05,
      "loss": 0.5178,
      "step": 1650
    },
    {
      "epoch": 0.983995257854179,
      "grad_norm": 290.25091552734375,
      "learning_rate": 3.360007903576369e-05,
      "loss": 0.4835,
      "step": 1660
    },
    {
      "epoch": 0.989922940130409,
      "grad_norm": 9.350057601928711,
      "learning_rate": 3.350128433115985e-05,
      "loss": 0.537,
      "step": 1670
    },
    {
      "epoch": 0.995850622406639,
      "grad_norm": 6.101749897003174,
      "learning_rate": 3.340248962655602e-05,
      "loss": 0.4872,
      "step": 1680
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.7745330566261488,
      "eval_f1": 0.7786992579659537,
      "eval_loss": 0.4941551387310028,
      "eval_precision": 0.6907589055240062,
      "eval_recall": 0.8922974324774925,
      "eval_runtime": 13.676,
      "eval_samples_per_second": 493.272,
      "eval_steps_per_second": 30.857,
      "step": 1687
    }
  ],
  "logging_steps": 10,
  "max_steps": 5061,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 161210619947520.0,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
